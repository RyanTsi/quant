import random
import numpy as np
import os
import pickle
import matplotlib.pyplot as plt
from datetime import datetime
import pandas as pd
from stable_baselines3 import SAC
from stable_baselines3.common.callbacks import CheckpointCallback, CallbackList, EvalCallback, BaseCallback
from stable_baselines3.common.utils import set_random_seed
from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
import gymnasium as gym
import torch
import torch.nn as nn

# --- è‡ªå®šä¹‰æ¨¡å—å¯¼å…¥ ---
import rl.prehandle
from rl.signalEnv import AStockSignalEnv
from database.influx_manager import InfluxDBManager, InfluxDBConfig, InfluxDBCallbacks
from config import * 

# ==========================================
# 1. æ—¥å¿—å›è°ƒ
# ==========================================
class DetailedLogCallback(BaseCallback):
    """
    ä» Env çš„ info ä¸­æå–è‡ªå®šä¹‰æŒ‡æ ‡å¹¶è®°å½•åˆ° TensorBoard
    """
    def _on_step(self) -> bool:
        # 1. è·å–å½“å‰ Step æ‰€æœ‰ç¯å¢ƒè¿”å›çš„ info (åˆ—è¡¨ï¼Œé•¿åº¦ä¸ºç¯å¢ƒæ•°é‡)
        infos = self.locals.get('infos', [])
        
        # 2. éå†ç¯å¢ƒ (é€šå¸¸ä½ ç”¨ DummyVecEnv åªæœ‰ä¸€ä¸ªç¯å¢ƒï¼Œä½†ä¸ºäº†é€šç”¨æ€§è¿™é‡Œç”¨å¾ªç¯)
        for info in infos:
            
            # --- A. è´¦æˆ·çŠ¶æ€ (æœ€æ ¸å¿ƒ) ---
            if 'State/Portfolio_Value' in info:
                # è®°å½•å‡€å€¼æ›²çº¿
                self.logger.record("main/Portfolio_Value", info['State/Portfolio_Value'])

            # --- B. è®­ç»ƒç›‘æ§ (Metrics) ---
            if 'Metrics/Raw_Alpha_Ret' in info:
                # åŸå§‹ Alpha æ”¶ç›Š (æœªæ‰£è´¹)
                self.logger.record("train/Raw_Alpha_Ret", info['Metrics/Raw_Alpha_Ret'])
            
            if 'Metrics/Cost' in info:
                # äº¤æ˜“æˆæœ¬æŸè€—
                self.logger.record("train/Transaction_Cost", info['Metrics/Cost'])
                
            if 'Metrics/Win_Rate_Step' in info:
                # èƒœç‡ (SB3 ä¼šè‡ªåŠ¨è®¡ç®— dump é—´éš”å†…çš„å¹³å‡å€¼)
                self.logger.record("train/Win_Rate", info['Metrics/Win_Rate_Step'])

            # --- C. å½’å› åˆ†æ (Attribution) ---
            # è¿™é‡Œçš„ç›®çš„æ˜¯çœ‹ï¼šä½ çš„æ”¶ç›Šåˆ°åº•æ¥è‡ªäº Alpha è¿˜æ˜¯å¤§ç›˜ Beta
            if 'Attribution/Alpha_Ret_Day' in info:
                self.logger.record("attribution/Alpha_Ret", info['Attribution/Alpha_Ret_Day'])
            
            if 'Attribution/Index_Ret_Day' in info:
                self.logger.record("attribution/Index_Ret", info['Attribution/Index_Ret_Day'])
                
            if 'Attribution/Abs_Ret_Day' in info:
                self.logger.record("attribution/Abs_Ret", info['Attribution/Abs_Ret_Day'])

            # --- D. è¡Œä¸ºè¯Šæ–­ (Behavior) ---
            # è§‚å¯Ÿæ¨¡å‹æ˜¯ä¸æ˜¯åªä¼šè¾“å‡º 0ï¼Œæˆ–è€…ç–¯ç‹‚è¾“å‡º 1/-1
            if 'Action/Signal' in info:
                self.logger.record("behavior/Signal_Mean", info['Action/Signal'])
                
            if 'Action/Confidence' in info:
                self.logger.record("behavior/Confidence", info['Action/Confidence'])

        return True

# ==========================================
# 2. æ•°æ®åŠ è½½å·¥å…· (å·²ä¿®æ”¹ï¼šç§»é™¤å¤–éƒ¨é¢„å¤„ç†)
# ==========================================
def get_data_with_cache(manager, codes, start_date, end_date, cache_name):
    """
    ä¿®æ”¹åçš„æ•°æ®åŠ è½½é€»è¾‘ï¼š
    1. ä»æ•°æ®åº“æ‹‰å–åŸå§‹æ•°æ®
    2. ä½¿ç”¨ rl.prehandle.preprocess_data è¿›è¡Œæ¸…æ´— (å‰”é™¤STã€æ­»è‚¡)
    3. åªæœ‰æ¸…æ´—åˆæ ¼çš„æ•°æ®æ‰è¿›å…¥åˆ—è¡¨
    """
    if os.path.exists(cache_name):
        print(f"ğŸ“¦ å‘ç°ç¼“å­˜ {cache_name}ï¼Œå¿«é€ŸåŠ è½½ä¸­...")
        with open(cache_name, "rb") as f:
            return pickle.load(f)
    
    print(f"ğŸš€ æœ¬åœ°æ— ç¼“å­˜ï¼Œå¼€å§‹ä¸‹è½½åŠæ¸…æ´— {len(codes)} åªè‚¡ç¥¨æ•°æ®...")
    df_list = []
    
    # å¿…é¡»ä¿è¯ index=0 æ˜¯å¤§ç›˜æŒ‡æ•°
    # æˆ‘ä»¬å‡è®¾ codes[0] æ˜¯ sh000001

    # å¤„ç†ä¸ªè‚¡ã€æŒ‡æ•°
    valid_count = 0
    skipped_count = 0
    
    for code in codes:
        try:
            df_temp = manager.get_stock_data_by_range(stock_code=code, start_time=start_date, end_time=end_date)
            
            # === è°ƒç”¨æ‚¨çš„æ¸…æ´—é€»è¾‘ ===
            # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¼ å…¥äº† codeï¼Œç”¨äºå‰ç¼€åˆ¤æ–­
            df_clean = rl.prehandle.preprocess_data(df_temp)
            
            if df_clean is not None:
                df_list.append(df_clean)
                valid_count += 1
            else:
                skipped_count += 1
                
        except Exception as e:
            print(f"âŒ {code} å¤„ç†å¼‚å¸¸: {e}")
            skipped_count += 1
            
        # è¿›åº¦æ‰“å°
        if (valid_count + skipped_count) % 500 == 0:
            print(f"å¤„ç†è¿›åº¦: æœ‰æ•ˆ {valid_count} / è·³è¿‡ {skipped_count} ...")
    
    print(f"ğŸ“Š æ•°æ®æ¸…æ´—å®Œæˆ: è¾“å…¥ {len(codes)-1} -> è¾“å‡º {valid_count} (å‰”é™¤ç‡ {skipped_count/(len(codes)-1):.1%})")

    if len(df_list) > 1: # è‡³å°‘è¦æœ‰ 1ä¸ªæŒ‡æ•° + 1ä¸ªè‚¡ç¥¨
        print(f"ğŸ’¾ ä¿å­˜ç¼“å­˜è‡³ {cache_name}...")
        with open(cache_name, "wb") as f:
            pickle.dump(df_list, f)
            
    return df_list

class LSTM_Attention_Extractor(BaseFeaturesExtractor):
    """
    å·¥ä¸šçº§æ—¶åºç‰¹å¾æå–å™¨
    ç»“æ„: Input -> LSTM -> (Attention) -> Linear -> Output to Policy
    """
    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 256):
        # åˆå§‹åŒ–çˆ¶ç±»ï¼Œfeatures_dim æ˜¯è¾“å‡ºç»™ SAC Actor/Critic çš„å‘é‡é•¿åº¦
        super().__init__(observation_space, features_dim)
        
        # 1. è‡ªåŠ¨æ¨æ–­è¾“å…¥ç»´åº¦
        # observation_space.shape é€šå¸¸æ˜¯ (Window_Size, Feature_Num)
        # ä¾‹å¦‚ (60, 5)
        self.window_size = observation_space.shape[0]
        self.input_features = observation_space.shape[1]
        
        # 2. å®šä¹‰ LSTM å±‚
        # hidden_size: éšå±‚ç»´åº¦ï¼Œè¶Šå¤§æ‹Ÿåˆèƒ½åŠ›è¶Šå¼ºï¼Œä½†è¶Šéš¾è®­ç»ƒ
        hidden_size = 64
        self.lstm = nn.LSTM(
            input_size=self.input_features,
            hidden_size=hidden_size,
            num_layers=2,           # å †å ä¸¤å±‚ LSTM æå–æ·±å±‚ç‰¹å¾
            batch_first=True,       # è¾“å…¥æ ¼å¼ (Batch, Seq, Feature)
            dropout=0.2             # é˜²æ­¢è¿‡æ‹Ÿåˆ
        )
        
        # 3. (å¯é€‰) ç®€å•çš„æ³¨æ„åŠ›æœºåˆ¶å±‚
        # ç”¨äºè®¡ç®— LSTM è¾“å‡ºåºåˆ—ä¸­æ¯ä¸ªæ—¶é—´æ­¥çš„æƒé‡
        self.attention = nn.Sequential(
            nn.Linear(hidden_size, 32),
            nn.Tanh(),
            nn.Linear(32, 1),
            nn.Softmax(dim=1)
        )
        
        # 4. æœ€ç»ˆæ˜ å°„å±‚
        # å°† LSTM/Attention çš„è¾“å‡ºæ˜ å°„åˆ° features_dim (256)
        self.linear = nn.Sequential(
            nn.Linear(hidden_size, features_dim),
            nn.LayerNorm(features_dim), # LayerNorm å¯¹é‡‘èæ—¶åºéå¸¸é‡è¦ï¼Œç¨³å®šæ¢¯åº¦
            nn.ReLU()
        )

    def forward(self, observations: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­é€»è¾‘
        observations shape: (Batch_Size, Window_Size, Features)
        """
        # 1. ç¡®ä¿è¾“å…¥æ˜¯ Float ç±»å‹
        x = observations.float()
        
        # 2. LSTM å‰å‘ä¼ æ’­
        # out: (Batch, Window, Hidden)
        # (h_n, c_n): æœ€åæ—¶åˆ»çš„éšçŠ¶æ€
        lstm_out, (h_n, c_n) = self.lstm(x)
        
        # --- ç­–ç•¥ A: ä»…ä½¿ç”¨æœ€åä¸€æ­¥ (ç»å…¸åšæ³•) ---
        # feature_vector = lstm_out[:, -1, :] 
        
        # --- ç­–ç•¥ B: ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶ (è¿›é˜¶åšæ³• - æ¨è) ---
        # è®¡ç®—æƒé‡: (Batch, Window, 1)
        weights = self.attention(lstm_out) 
        # åŠ æƒæ±‚å’Œ: (Batch, Hidden)
        # è¿™é‡Œçš„å«ä¹‰æ˜¯ï¼šæ¨¡å‹è‡ªåŠ¨å­¦ä¼šè¿™60å¤©é‡Œï¼Œå“ªå‡ å¤©å¯¹é¢„æµ‹T+1æœ€é‡è¦
        context_vector = torch.sum(weights * lstm_out, dim=1)
        
        # 3. æœ€ç»ˆæ˜ å°„
        return self.linear(context_vector)
    

# ==========================================
# 3. ä¸»ç¨‹åº
# ==========================================
SEED = 541438
ADDITIONAL_STEPS = 2_000_000 

if __name__ == "__main__":
    set_random_seed(SEED)
    
    # --- A. æ•°æ®å‡†å¤‡ ---
    # ç¡®ä¿ config.py ä¸­å®šä¹‰äº† HOST, DATABASE, TOKEN ç­‰
    config_db = InfluxDBConfig(HOST, DATABASE, TOKEN)
    manager = InfluxDBManager(config_db, InfluxDBCallbacks())
    
    # è·å–è‚¡ç¥¨åˆ—è¡¨
    target_date = datetime(2023, 12, 12)
    all_codes = manager.get_stock_code_list_by_date(target_date)
    
    # ç¡®ä¿å¤§ç›˜æŒ‡æ•°åœ¨ç¬¬ä¸€ä½
    index_code = "sh000001"
    if index_code in all_codes:
        all_codes.remove(index_code)
    all_codes.insert(0, index_code)

    # åŠ è½½æ•°æ® (Train/Val/Test)
    # è¿™é‡Œçš„ train_range ç­‰å˜é‡éœ€åœ¨ config.py ä¸­å®šä¹‰
    print("æ­£åœ¨åŠ è½½è®­ç»ƒé›†...")
    train_dfs = get_data_with_cache(manager, all_codes, train_range[0], train_range[1], "train_data_v4.pkl")
    print("æ­£åœ¨åŠ è½½éªŒè¯é›†...")
    val_dfs   = get_data_with_cache(manager, all_codes, val_range[0], val_range[1], "val_data_v4.pkl")
    
    manager.close()

    # --- B. ç¯å¢ƒæ„å»º ---
    # æ³¨æ„ï¼šä½¿ç”¨ v4.0 çš„å‚æ•°é…ç½®
    env_kwargs = {
        'window_size': 60,
        'training_days': 252,
        'transaction_cost_pct': 0.0000,
        'deadzone_level': 0.1,
        'reward_scale': 1
    }

    print("æ„å»ºè®­ç»ƒç¯å¢ƒ...")
    train_env = DummyVecEnv([lambda: AStockSignalEnv(train_dfs, **env_kwargs)])
    train_env = VecMonitor(train_env, TRAIN_LOG_DIR)

    print("æ„å»ºéªŒè¯ç¯å¢ƒ...")
    val_env = DummyVecEnv([lambda: AStockSignalEnv(val_dfs, **env_kwargs)])
    val_env = VecMonitor(val_env, VAL_LOG_DIR)

    # --- C. å›è°ƒå‡½æ•°ç»„è£… ---
    
    # 1. éªŒè¯å›è°ƒ
    eval_callback = EvalCallback(
        val_env,
        best_model_save_path='./best_modelV4/',
        log_path=VAL_LOG_DIR,
        eval_freq=5_000,        # ç¨å¾®é™ä½é¢‘ç‡ï¼ŒåŠ å¿«è®­ç»ƒé€Ÿåº¦
        n_eval_episodes=20,     # éªŒè¯20ä¸ªEpisode (20åªéšæœºè‚¡ç¥¨/æ—¶é—´æ®µ)
        deterministic=True,
        render=False
    )
    
    # 2. æ£€æŸ¥ç‚¹å›è°ƒ
    checkpoint_callback = CheckpointCallback(
        save_freq=5_000, 
        save_path='./checkpoints_v4/', 
        name_prefix='sac_v4'
    )
    
    # 3. è¯¦ç»†æ—¥å¿—å›è°ƒ
    log_callback = DetailedLogCallback()

    callback_list = CallbackList([eval_callback, checkpoint_callback, log_callback])

    # --- D. æ¨¡å‹åŠ è½½ä¸è®­ç»ƒ ---
    best_model_path = "./best_modelV4/best_model.zip"
    
    if os.path.exists(best_model_path):
        print(f"ğŸ”„ å‘ç°ç°æœ‰æ¨¡å‹ {best_model_path}ï¼Œæ­£åœ¨åŠ è½½...")
        model = SAC.load(best_model_path, env=train_env, device="cuda")
        
        # å°è¯•åŠ è½½ Replay Buffer
        buffer_path = "./best_modelV4/replay_buffer.pkl"
        if os.path.exists(buffer_path):
            try:
                print("ğŸ’¾ åŠ è½½ Replay Buffer...")
                model.load_replay_buffer(buffer_path)
            except Exception as e:
                print(f"âš ï¸ Buffer åŠ è½½å¤±è´¥ (å¯èƒ½æ˜¯Obs Shapeå˜äº†): {e}")
                
        print(f"ğŸ“ˆ ç»§ç»­è®­ç»ƒï¼Œç›®æ ‡æ­¥æ•°: {ADDITIONAL_STEPS}")
        model.learn(total_timesteps=ADDITIONAL_STEPS, callback=callback_list, reset_num_timesteps=False)
        
    else:
        print("ğŸ†• åˆ›å»ºå…¨æ–° SAC æ¨¡å‹ (V4 Environment)...")
        policy_kwargs = dict(
            # 1. æŒ‡å®šè‡ªå®šä¹‰æå–å™¨
            features_extractor_class=LSTM_Attention_Extractor,
            
            # 2. ä¼ é€’å‚æ•°ç»™æå–å™¨ (å¯¹åº” __init__ ä¸­çš„å‚æ•°)
            features_extractor_kwargs=dict(features_dim=256),
            
            # 3. å®šä¹‰æå–å™¨ä¹‹åçš„ç½‘ç»œç»“æ„ (Actor å’Œ Critic)
            # å› ä¸º LSTM å·²ç»æå–äº†å¼ºåŠ›çš„ç‰¹å¾ï¼Œåé¢çš„ç½‘ç»œå¯ä»¥ç¨å¾®ç®€å•ç‚¹
            net_arch=dict(pi=[128, 64], qf=[128, 64]),
            
            # 4. ä¼˜åŒ–å™¨å‚æ•° (å¯é€‰ï¼Œå¾®è°ƒ)
            optimizer_kwargs=dict(weight_decay=1e-5) # L2 æ­£åˆ™åŒ–ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
        )
        # é’ˆå¯¹é‡‘èæ—¶é—´åºåˆ—è°ƒæ•´çš„ SAC å‚æ•°
        model = SAC(
            "MlpPolicy", 
            train_env, 
            verbose=1, 
            tensorboard_log=TRAIN_LOG_DIR,
            device="cuda",
            buffer_size=500_000,
            learning_starts=20_000,
            batch_size=512,
            ent_coef='auto',
            # policy_kwargs=dict(net_arch=[256, 256])
            policy_kwargs=policy_kwargs
        )
        
        print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
        model.learn(total_timesteps=ADDITIONAL_STEPS, callback=callback_list)

    # --- E. ä¿å­˜æœ€ç»ˆç»“æœ ---
    print("âœ… è®­ç»ƒç»“æŸã€‚ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
    model.save("./best_modelV4/final_model")
    try:
        model.save_replay_buffer("./best_modelV4/replay_buffer.pkl")
    except Exception as e:
        print(f"Bufferä¿å­˜å¤±è´¥: {e}")