import random
import numpy as np
import os
import pickle
import matplotlib.pyplot as plt  # å¼•å…¥ç»˜å›¾åº“
from datetime import datetime
from stable_baselines3 import SAC
from stable_baselines3.common.callbacks import CheckpointCallback, CallbackList, EvalCallback, BaseCallback
from stable_baselines3.common.utils import set_random_seed
from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor
import torch

# --- è‡ªå®šä¹‰æ¨¡å—å¯¼å…¥ ---
import rl.prehandle
from rl.environment import SimpleStockEnv
from database.influx_manager import InfluxDBManager, InfluxDBConfig, InfluxDBCallbacks
from config import * 

# ==========================================
# 1. æ ¸å¿ƒç»„ä»¶ï¼šè‡ªé€‚åº” Alpha å›è°ƒå‡½æ•°
# ==========================================
class AdaptiveAlphaCallback(BaseCallback):
    """
    æ ¹æ®æ¨¡å‹è¡¨ç°è‡ªåŠ¨è°ƒæ•´ Alpha (é£é™©æƒ©ç½šæƒé‡)
    é€»è¾‘ï¼šå½“å¹³å‡å¥–åŠ±è¶…è¿‡é˜ˆå€¼ï¼Œä¸”è¿‡äº†å†·å´æœŸï¼Œå°±å¢åŠ  Alpha
    """
    def __init__(self, verbose=0, start_alpha=0.1, max_alpha=1.8, cooldown_steps=30000, warmup_steps=61000):
        super(AdaptiveAlphaCallback, self).__init__(verbose)
        self.current_alpha = start_alpha
        self.max_alpha = max_alpha
        self.reward_threshold = 0.5  # åˆå§‹é—¨æ§›ï¼šå¹³å‡æ”¶ç›Šè¾¾åˆ° 0.5 æ‰åŠ å‹
        self.warmup_steps = warmup_steps
        self.last_update_step = 0
        self.cooldown = cooldown_steps
        self.step_size = 0.2         # æ¯æ¬¡å¢åŠ  0.2

    def _on_step(self) -> bool:
        global_step = self.num_timesteps
        
        # è·å–æœ€è¿‘ 100 å±€çš„å¹³å‡å¥–åŠ± (SB3 è‡ªåŠ¨ç»´æŠ¤è¯¥æŒ‡æ ‡)
        # è¿™é‡Œçš„ key å¿…é¡»æ˜¯ SB3 åŸç”Ÿè®°å½•çš„ rollout/ep_rew_mean
        ep_rew_mean = self.logger.name_to_value.get("rollout/ep_rew_mean", -np.inf)

        # --- è‡ªé€‚åº”åˆ¤æ–­é€»è¾‘ ---
        # 1. è¶…è¿‡é¢„çƒ­
        # 2. å¥–åŠ±è¶…è¿‡å½“å‰é—¨æ§›
        # 3. è·ç¦»ä¸Šæ¬¡è°ƒæ•´å·²ç»è¿‡äº†å†·å´æœŸ (é˜²æ­¢é¢‘ç¹éœ‡è¡)
        # 4. Alpha è¿˜æ²¡åˆ°ä¸Šé™
        if (global_step > self.warmup_steps and
            ep_rew_mean > self.reward_threshold and 
            (global_step - self.last_update_step) > self.cooldown and 
            self.current_alpha < self.max_alpha):
            
            # æ‰§è¡Œå‡çº§
            self.current_alpha += self.step_size
            self.reward_threshold += 0.5  # æé«˜ä¸‹ä¸€æ¬¡çš„é—¨æ§›ï¼Œé€¼è¿«æ¨¡å‹è¿›åŒ–
            self.last_update_step = global_step
            
            # æ³¨å…¥ç¯å¢ƒ (DummyVecEnv ä¸‹ set_attr æ˜¯å³æ—¶ç”Ÿæ•ˆçš„)
            self.training_env.set_attr("alpha", self.current_alpha)
            
            print(f"\nğŸ”¥ [è¿›åŒ–æ—¶åˆ»] Step {global_step}: Alpha æå‡è‡³ {self.current_alpha:.1f}, ä¸‹ä¸€ç›®æ ‡ Reward > {self.reward_threshold:.1f}")

        # --- Tensorboard è®°å½• ---
        # è®°å½•ç¯å¢ƒå‚æ•°å˜åŒ–
        self.logger.record("env/adaptive_alpha", self.current_alpha)
        self.logger.record("env/target_threshold", self.reward_threshold)
        
        # è®°å½•å…³é”®æ€§èƒ½æŒ‡æ ‡ (ä» Info ä¸­æå–)
        if len(self.locals['infos']) > 0:
            info = self.locals['infos'][0]
            if "net_worth" in info:
                self.logger.record("performance/net_worth", info["net_worth"])
            if "max_drawdown" in info:
                self.logger.record("performance/max_drawdown", info["max_drawdown"])
            if "pos_ratio" in info:
                self.logger.record("performance/position_ratio", info["pos_ratio"])
                self.logger.record("performance/alpha", info["alpha"])
            
            # è®°å½•å¥–åŠ±ç»†èŠ‚
            reward_keys = ["ave_r_base", "ave_r_risk", "max_r_base", "max_r_risk"]
            for key in reward_keys:
                if key in info:
                    self.logger.record(f"rewards/{key}", info[key])

        return True

# ==========================================
# 2. æ•°æ®åŠ è½½å·¥å…· (å¸¦ç¼“å­˜)
# ==========================================
def get_data_with_cache(manager, codes, start_date, end_date, cache_name):
    """ä¼˜å…ˆä»æœ¬åœ° pickle è¯»å–ï¼Œå¦åˆ™ä» InfluxDB ä¸‹è½½å¹¶ç¼“å­˜"""
    if os.path.exists(cache_name):
        print(f"ğŸ“¦ å‘ç°ç¼“å­˜ {cache_name}ï¼Œå¿«é€ŸåŠ è½½ä¸­...")
        with open(cache_name, "rb") as f:
            return pickle.load(f)
    
    print(f"ğŸš€ æœ¬åœ°æ— ç¼“å­˜ï¼Œå¼€å§‹ä¸‹è½½ {len(codes)} åªè‚¡ç¥¨æ•°æ®...")
    df_list = []
    for code in codes:
        try:
            df_temp = manager.get_stock_data_by_range(stock_code=code, start_time=start_date, end_time=end_date)
            df_clean = rl.prehandle.preprocess_data(df_temp)
            # ç®€å•è¿‡æ»¤ï¼šæ•°æ®é•¿åº¦ä¸å¤Ÿçš„ä¸è¦
            if df_clean is not None and len(df_clean) > WINDOW_SIZE + 200:
                df_list.append(df_clean)
        except Exception as e:
            print(f"âŒ {code} å¤±è´¥: {e}")
    
    if df_list:
        print(f"ğŸ’¾ ä¿å­˜ç¼“å­˜è‡³ {cache_name}...")
        with open(cache_name, "wb") as f:
            pickle.dump(df_list, f)
            
    return df_list

# ==========================================
# 3. ä¸»ç¨‹åº
# ==========================================
SEED = 5418
ADDITIONAL_STEPS = 2_000_000

if __name__ == "__main__":
    set_random_seed(SEED)
    
    # --- A. æ•°æ®å‡†å¤‡ ---
    config = InfluxDBConfig(HOST, DATABASE, TOKEN)
    manager = InfluxDBManager(config, InfluxDBCallbacks())
    
    # è·å–è‚¡ç¥¨åˆ—è¡¨
    target_date = datetime(2025, 12, 12)
    all_codes = manager.get_stock_code_list_by_date(target_date)
    # è¿‡æ»¤ä¸»æ¿
    valid_prefixes = ('600', '601', '603', '000', '002')
    main_codes = [c for c in all_codes if c.startswith(valid_prefixes)]
    
    # éšæœºæŠ½ 1200 åª
    selected_codes = np.random.choice(main_codes, size=min(1200, len(main_codes)), replace=False)
    print(f"ğŸ“Š é€‰ä¸­è‚¡ç¥¨æ•°é‡: {len(selected_codes)}")

    train_dfs = get_data_with_cache(manager, selected_codes, train_range[0], train_range[1], "train_data.pkl")
    val_dfs   = get_data_with_cache(manager, selected_codes, val_range[0], val_range[1], "val_data.pkl")
    test_dfs  = get_data_with_cache(manager, selected_codes, test_range[0], test_range[1], "test_data.pkl")
    manager.close()

    # --- B. ç¯å¢ƒæ„å»º (å•è¿›ç¨‹ DummyVecEnv) ---
    train_env = DummyVecEnv([lambda: SimpleStockEnv(train_dfs)])
    train_env = VecMonitor(train_env, TRAIN_LOG_DIR)

    val_env = DummyVecEnv([lambda: SimpleStockEnv(val_dfs)])
    val_env = VecMonitor(val_env, VAL_LOG_DIR)

    # --- C. å›è°ƒå‡½æ•°ç»„è£… ---
    
    # 1. éªŒè¯å›è°ƒï¼šå®šæœŸåœ¨éªŒè¯é›†ä¸Šæµ‹è¯•
    eval_callback = EvalCallback(
        val_env,
        best_model_save_path='./best_model/',
        log_path=VAL_LOG_DIR,
        eval_freq=10_000,
        n_eval_episodes=100,     # éªŒè¯ 100 å±€
        deterministic=True,
        render=False
    )
    
    # 2. æ£€æŸ¥ç‚¹å›è°ƒï¼šå®šæœŸä¿å­˜æ¨¡å‹æ–‡ä»¶
    checkpoint_callback = CheckpointCallback(
        save_freq=20000, 
        save_path='./checkpoints/', 
        name_prefix='sac_adaptive'
    )
    
    # 3. è‡ªé€‚åº” Alpha å›è°ƒï¼šæ ¸å¿ƒé€»è¾‘
    # å‡è®¾æˆ‘ä»¬ä» 0.0 å¼€å§‹ï¼Œæœ€é«˜åˆ° 1.8
    adaptive_cb = AdaptiveAlphaCallback(start_alpha=0.1, max_alpha=1.8)

    callback_list = CallbackList([eval_callback, checkpoint_callback, adaptive_cb])

    # --- D. æ¨¡å‹åŠ è½½ä¸è®­ç»ƒ (æ–­ç‚¹ç»­ä¼ æ ¸å¿ƒ) ---
    best_model_path = "./best_model/best_model.zip"
    policy_kwargs = dict(
        net_arch=dict(pi=[256, 256], qf=[256, 256]), # ç½‘ç»œå¤§å°é€‚ä¸­
        activation_fn=torch.nn.ReLU
    )
    if os.path.exists(best_model_path):
        print(f"ğŸ”„ å‘ç°ç°æœ‰æ¨¡å‹ {best_model_path}ï¼Œæ­£åœ¨åŠ è½½...")
        model = SAC.load(best_model_path, env=train_env, device="cuda")
        # è®¡ç®—æ–°çš„ç›®æ ‡æ­¥æ•°
        current_steps = model.num_timesteps
        target_steps = current_steps + ADDITIONAL_STEPS
        print(f"ğŸ“ˆ å†å²æ­¥æ•°: {current_steps}")
        print(f"ğŸ¯ ç›®æ ‡æ­¥æ•°: {target_steps} (+{ADDITIONAL_STEPS})")
        
        # å°è¯•åŠ è½½ Replay Buffer (å¦‚æœå­˜åœ¨)ï¼Œè¿™ä¼šè®©è®­ç»ƒæ›´å¹³æ»‘
        buffer_path = "./best_model/replay_buffer.pkl"
        if os.path.exists(buffer_path):
            print("ğŸ’¾ åŠ è½½ Replay Buffer...")
            model.load_replay_buffer(buffer_path)
            
    else:
        print("ğŸ†• åˆ›å»ºå…¨æ–° SAC æ¨¡å‹...")
        model = SAC(
            "MlpPolicy", 
            train_env, 
            verbose=1, 
            tensorboard_log=TRAIN_LOG_DIR,
            device="cuda",
            policy_kwargs=policy_kwargs,
            buffer_size=1_000_000,
            learning_starts=60_000, # é¢„æ”¶é›†ï¼šå…ˆè·‘ 2ä¸‡æ­¥ (çº¦300å±€) 
            batch_size=4096,        # å¤§ Batchï¼šä¸€æ¬¡çœ‹ 4096 æ¡æ•°æ®
            tau=0.005,
            gamma=0.99,
            learning_rate=1e-4,
            train_freq=7,
            gradient_steps=1,
            ent_coef='auto',
        )
        target_steps = ADDITIONAL_STEPS

    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    try:
        model.learn(
            total_timesteps=target_steps, 
            callback=callback_list,
            reset_num_timesteps=False
        )
    except KeyboardInterrupt:
        print("âš ï¸ æ£€æµ‹åˆ°ä¸­æ–­ï¼Œæ­£åœ¨ä¿å­˜å½“å‰æ¨¡å‹...")
        model.save("./best_model/interrupted_model")
        # æ‰‹åŠ¨ä¿å­˜ buffer
        model.save_replay_buffer("./best_model/replay_buffer.pkl")

    print("âœ… è®­ç»ƒç»“æŸã€‚ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
    model.save("./best_model/final_model")
    model.save_replay_buffer("./best_model/replay_buffer.pkl")

    # --- E. æœ€ç»ˆå›æµ‹ä¸å¯è§†åŒ– ---
    print("\nğŸ” å¼€å§‹å›æµ‹å¯è§†åŒ–...")
    
    # åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæµ‹è¯•
    test_model = SAC.load("./best_model/best_model.zip", device="cuda")
    test_env = DummyVecEnv([lambda: SimpleStockEnv(test_dfs)]) # ä½¿ç”¨æµ‹è¯•é›†
    
    returns = []
    obs = test_env.reset()
    
    # æµ‹è¯• 100 ä¸ªEpisode
    for i in range(100):
        done = False
        while not done:
            action, _ = test_model.predict(obs, deterministic=True)
            obs, reward, done, info = test_env.step(action)
            
            if done:
                # æå–æ”¶ç›Šç‡
                net_worth = info[0]["net_worth"]
                roi = (net_worth - ORIGINAL_MONEY) / ORIGINAL_MONEY
                returns.append(roi)
                print(f"æµ‹è¯•å±€ {i+1}: æ”¶ç›Šç‡ {roi*100:.2f}%")

    # ç»˜åˆ¶ç›´æ–¹å›¾
    plt.figure(figsize=(10, 6))
    plt.hist(returns, bins=20, color='skyblue', edgecolor='black', alpha=0.7)
    plt.axvline(x=0, color='r', linestyle='--', label='ç›ˆäºå¹³è¡¡çº¿')
    plt.title('æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ”¶ç›Šåˆ†å¸ƒ (100å±€)')
    plt.xlabel('æ”¶ç›Šç‡ (ROI)')
    plt.ylabel('é¢‘æ¬¡')
    plt.legend()
    plt.grid(axis='y', alpha=0.5)
    
    # ä¿å­˜å›¾ç‰‡
    plot_path = os.path.join(TRAIN_LOG_DIR, "backtest_distribution.png")
    plt.savefig(plot_path)
    print(f"ğŸ“Š æ”¶ç›Šåˆ†å¸ƒå›¾å·²ä¿å­˜è‡³: {plot_path}")
    
    # æ‰“å°ç»Ÿè®¡æ•°æ®
    returns = np.array(returns)
    print(f"\nğŸ† æœ€ç»ˆæˆç»©å•:")
    print(f"å¹³å‡æ”¶ç›Š: {np.mean(returns)*100:.2f}%")
    print(f"æ­£æ”¶ç›Šæ¯”ä¾‹: {np.sum(returns > 0)} / {len(returns)} ({np.sum(returns > 0)/len(returns)*100:.0f}%)")
    print(f"æœ€å¤§å•å±€ç›ˆåˆ©: {np.max(returns)*100:.2f}%")
    print(f"æœ€å¤§å•å±€äºæŸ: {np.min(returns)*100:.2f}%")