import random
import numpy as np
import os
import pickle
import matplotlib.pyplot as plt
from datetime import datetime
from stable_baselines3 import SAC
from stable_baselines3.common.callbacks import CheckpointCallback, CallbackList, EvalCallback, BaseCallback
from stable_baselines3.common.utils import set_random_seed
from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor, SubprocVecEnv
import torch

# --- è‡ªå®šä¹‰æ¨¡å—å¯¼å…¥ ---
import rl.prehandle
from rl.environment import SimpleStockEnv
from database.influx_manager import InfluxDBManager, InfluxDBConfig, InfluxDBCallbacks
from config import * 

# ==========================================
# 1. æ ¸å¿ƒç»„ä»¶ï¼šè¯¦ç»†æ—¥å¿—å›è°ƒ
# ==========================================
class DetailedLogCallback(BaseCallback):
    """
    ä¸“é—¨ç”¨äºè®°å½•æ–°ç¯å¢ƒç‰¹æ€§çš„æ—¥å¿—å›è°ƒ
    ä¸å†æ§åˆ¶ Alphaï¼Œè€Œæ˜¯è§‚å¯Ÿ Agent åœ¨ä¸åŒ Alpha ä¸‹çš„è¡¨ç°ï¼Œä»¥åŠç°é‡‘å¥–åŠ±çš„è·å–æƒ…å†µ
    """
    def __init__(self, verbose=0):
        super(DetailedLogCallback, self).__init__(verbose)

    def _on_step(self) -> bool:
        # è®°å½•å…³é”®æ€§èƒ½æŒ‡æ ‡ (ä» Info ä¸­æå–)
        # SB3 çš„ VecEnv ä¼šè‡ªåŠ¨å †å  Infoï¼Œè¿™é‡Œå–ç¬¬ä¸€ä¸ªç¯å¢ƒçš„ Info
        if len(self.locals['infos']) > 0:
            info = self.locals['infos'][0]
            
            # --- 1. è®°å½• Reward ç»„æˆ (éªŒè¯ç°é‡‘å¥–åŠ±æœºåˆ¶) ---
            # è¿™é‡Œçš„ key è¦å¯¹åº”ç¯å¢ƒ info ä¸­çš„ key
            if "ave_r_base" in info:
                self.logger.record("rewards/1_base_return", info["ave_r_base"])
            if "ave_r_base_pos" in info:
                self.logger.record("rewards/2_base_pos_return", info["ave_r_base_pos"])
            if "ave_r_base_neg" in info:
                self.logger.record("rewards/3_base_neg_return", info["ave_r_base_neg"])
            if "ave_r_cash" in info:
                self.logger.record("rewards/4_cash_interest", info["ave_r_cash"])
            if "ave_r_risk" in info:
                self.logger.record("rewards/5_risk_penalty", info["ave_r_risk"])
            if "ave_r_turnover" in info:
                self.logger.record("rewards/6_turnover_penalty", info["ave_r_turnover"])

            # --- 2. è®°å½•èµ„äº§çŠ¶æ€ ---
            if "net_worth" in info:
                self.logger.record("status/net_worth", info["net_worth"])
            if "max_dd" in info:
                self.logger.record("status/max_drawdown", info["max_dd"])
            if "alpha" in info:
                self.logger.record("status/current_alpha", info["alpha"])
            if "price" in info:
                self.logger.record("status/price", info["price"])

        return True

# ==========================================
# 2. æ•°æ®åŠ è½½å·¥å…· (ä¿æŒä¸å˜)
# ==========================================
def get_data_with_cache(manager, codes, start_date, end_date, cache_name):
    """ä¼˜å…ˆä»æœ¬åœ° pickle è¯»å–ï¼Œå¦åˆ™ä» InfluxDB ä¸‹è½½å¹¶ç¼“å­˜"""
    if os.path.exists(cache_name):
        print(f"ğŸ“¦ å‘ç°ç¼“å­˜ {cache_name}ï¼Œå¿«é€ŸåŠ è½½ä¸­...")
        with open(cache_name, "rb") as f:
            return pickle.load(f)
    
    print(f"ğŸš€ æœ¬åœ°æ— ç¼“å­˜ï¼Œå¼€å§‹ä¸‹è½½ {len(codes)} åªè‚¡ç¥¨æ•°æ®...")
    df_list = []
    for code in codes:
        try:
            df_temp = manager.get_stock_data_by_range(stock_code=code, start_time=start_date, end_time=end_date)
            df_clean = rl.prehandle.preprocess_data(df_temp)
            if df_clean is not None and len(df_clean) > WINDOW_SIZE + 200:
                df_list.append(df_clean)
        except Exception as e:
            print(f"âŒ {code} å¤±è´¥: {e}")
    
    if df_list:
        print(f"ğŸ’¾ ä¿å­˜ç¼“å­˜è‡³ {cache_name}...")
        with open(cache_name, "wb") as f:
            pickle.dump(df_list, f)
            
    return df_list

# ==========================================
# 3. ä¸»ç¨‹åº
# ==========================================
SEED = 541438
ADDITIONAL_STEPS = 2_000_000 # è®­ç»ƒæ­¥æ•°

if __name__ == "__main__":
    set_random_seed(SEED)
    
    # --- A. æ•°æ®å‡†å¤‡ ---
    # ç¡®ä¿ config.py ä¸­æœ‰ train_range, val_range ç­‰å®šä¹‰
    config = InfluxDBConfig(HOST, DATABASE, TOKEN)
    manager = InfluxDBManager(config, InfluxDBCallbacks())
    
    # è·å–è‚¡ç¥¨åˆ—è¡¨
    target_date = datetime(2025, 12, 12) # æ³¨æ„ï¼šè¿™ä¸ªæ—¥æœŸéœ€è¦ç¡®ä¿åœ¨æ•°æ®åº“èŒƒå›´å†…
    # å¦‚æœæ˜¯å›æµ‹ï¼Œé€šå¸¸ç”¨è¿‡å»çš„æ—¶é—´ï¼›å¦‚æœæ˜¯æ¨¡æ‹Ÿï¼Œç¡®ä¿èƒ½å–åˆ°ä»£ç è¡¨
    try:
        all_codes = manager.get_stock_code_list_by_date(target_date)
    except:
        # å¦‚æœå–ä¸åˆ°ï¼Œç”¨ä¸€ä¸ªå…œåº•é€»è¾‘æˆ–è€…å–æœ€æ–°
        print("âš ï¸ æ— æ³•è·å–æŒ‡å®šæ—¥æœŸä»£ç ï¼Œå°è¯•è·å–æ‰€æœ‰...")
        # è¿™é‡Œéœ€è¦ä½ è‡ªå·±æ ¹æ®æ•°æ®åº“æ¥å£è°ƒæ•´ï¼Œå‡è®¾è·å–æˆåŠŸ
        all_codes = [] 

    valid_prefixes = ('600', '601', '603', '000', '002')
    main_codes = [c for c in all_codes if c.startswith(valid_prefixes)]
    
    # éšæœºæŠ½æ ·
    selected_codes = np.random.choice(main_codes, size=min(1200, len(main_codes)), replace=False)
    print(f"ğŸ“Š é€‰ä¸­è‚¡ç¥¨æ•°é‡: {len(selected_codes)}")

    train_dfs = get_data_with_cache(manager, selected_codes, train_range[0], train_range[1], "train_data.pkl")
    val_dfs   = get_data_with_cache(manager, selected_codes, val_range[0], val_range[1], "val_data.pkl")
    test_dfs  = get_data_with_cache(manager, selected_codes, test_range[0], test_range[1], "test_data.pkl")
    manager.close()

    # --- B. ç¯å¢ƒæ„å»º ---
    train_env = DummyVecEnv([lambda: SimpleStockEnv(train_dfs)])
    train_env = VecMonitor(train_env, TRAIN_LOG_DIR)

    val_env = DummyVecEnv([lambda: SimpleStockEnv(val_dfs)])
    val_env = VecMonitor(val_env, VAL_LOG_DIR)

    # --- C. å›è°ƒå‡½æ•°ç»„è£… ---
    
    # 1. éªŒè¯å›è°ƒ
    eval_callback = EvalCallback(
        val_env,
        best_model_save_path='./best_model/',
        log_path=VAL_LOG_DIR,
        eval_freq=1_000,
        n_eval_episodes=50,     
        deterministic=True,
        render=False
    )
    
    # 2. æ£€æŸ¥ç‚¹å›è°ƒ
    checkpoint_callback = CheckpointCallback(
        save_freq=50000, 
        save_path='./checkpoints/', 
        name_prefix='sac_v2'
    )
    
    # 3. è¯¦ç»†æ—¥å¿—å›è°ƒ
    log_callback = DetailedLogCallback()

    callback_list = CallbackList([eval_callback, checkpoint_callback, log_callback])

    # --- D. æ¨¡å‹åŠ è½½ä¸è®­ç»ƒ ---
    best_model_path = "./best_model/best_model.zip"
    
    # ç½‘ç»œæ¶æ„ï¼šå¯ä»¥é€‚å½“åŠ å®½ï¼Œä»¥å¤„ç†æ›´å¤æ‚çš„çŠ¶æ€ï¼ˆAlphaè¾“å…¥ï¼‰
    policy_kwargs = dict(
        net_arch=dict(pi=[128, 128], qf=[128, 128]),
        activation_fn=torch.nn.ReLU
    )
    
    if os.path.exists(best_model_path):
        print(f"ğŸ”„ å‘ç°ç°æœ‰æ¨¡å‹ {best_model_path}ï¼Œæ­£åœ¨åŠ è½½...")
        # custom_objects ç”¨äºå¤„ç†ç‰ˆæœ¬å…¼å®¹æ€§æˆ–ç‰¹å®šå‚æ•°å˜åŒ–
        model = SAC.load(best_model_path, env=train_env, device="cuda")
        
        current_steps = model.num_timesteps
        target_steps = current_steps + ADDITIONAL_STEPS
        print(f"ğŸ“ˆ å†å²æ­¥æ•°: {current_steps}")
        print(f"ğŸ¯ ç›®æ ‡æ­¥æ•°: {target_steps} (+{ADDITIONAL_STEPS})")
        
        # å°è¯•åŠ è½½ Buffer
        buffer_path = "./best_model/replay_buffer.pkl"
        if os.path.exists(buffer_path):
            try:
                print("ğŸ’¾ åŠ è½½ Replay Buffer...")
                model.load_replay_buffer(buffer_path)
            except Exception as e:
                print(f"âš ï¸ Buffer åŠ è½½å¤±è´¥ (å¯èƒ½æ˜¯ç¯å¢ƒObsç©ºé—´å˜äº†): {e}")
                print("âš ï¸ å°†ä½¿ç”¨ç©º Buffer ç»§ç»­è®­ç»ƒ")
    else:
        print("ğŸ†• åˆ›å»ºå…¨æ–° SAC æ¨¡å‹ (V2 Environment)...")
        model = SAC(
            "MlpPolicy", 
            train_env, 
            verbose=1, 
            tensorboard_log=TRAIN_LOG_DIR,
            device="cuda",
            policy_kwargs=policy_kwargs,
            buffer_size=1_000_000,
            learning_starts=10_000, 
            batch_size=4096,        
            tau=0.005,
            gamma=0.99,
            learning_rate=3e-4, # ç¨å¾®è°ƒå¤§ä¸€ç‚¹åˆå§‹ LRï¼Œå› ä¸ºæœ‰äº† Cash Reward å®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜
            train_freq=20,       # å¢åŠ æ›´æ–°é¢‘ç‡
            gradient_steps=20,
            ent_coef='auto',
        )
        target_steps = ADDITIONAL_STEPS

    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    try:
        model.learn(
            total_timesteps=target_steps, 
            callback=callback_list,
            reset_num_timesteps=False
        )
    except KeyboardInterrupt:
        print("âš ï¸ æ£€æµ‹åˆ°ä¸­æ–­ï¼Œæ­£åœ¨ä¿å­˜å½“å‰æ¨¡å‹...")
        model.save("./best_model/interrupted_model")
        # æ‰‹åŠ¨ä¿å­˜ buffer
        model.save_replay_buffer("./best_model/replay_buffer.pkl")

    print("âœ… è®­ç»ƒç»“æŸã€‚ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
    model.save("./best_model/final_model")
    model.save_replay_buffer("./best_model/replay_buffer.pkl")

    # --- E. æœ€ç»ˆå›æµ‹ä¸å¯è§†åŒ– ---
    print("\nğŸ” å¼€å§‹å›æµ‹å¯è§†åŒ– (æµ‹è¯•é›†)...")
    
    test_model = SAC.load("./best_model/best_model.zip", device="cuda")
    test_env = DummyVecEnv([lambda: SimpleStockEnv(test_dfs)]) 
    
    returns = []
    alphas = [] # è®°å½•æ¯ä¸€å±€çš„ Alpha
    
    obs = test_env.reset()
    
    # æµ‹è¯• 100 ä¸ªEpisode
    for i in range(100):
        done = False
        while not done:
            action, _ = test_model.predict(obs, deterministic=True)
            obs, reward, done, info_list = test_env.step(action)
            
            if done:
                info = info_list[0]
                net_worth = info["net_worth"]
                roi = (net_worth - ORIGINAL_MONEY) / ORIGINAL_MONEY
                returns.append(roi)
                alphas.append(info["alpha"])
                
                print(f"æµ‹è¯•å±€ {i+1} | Alpha: {info['alpha']:.2f} | æ”¶ç›Šç‡: {roi*100:.2f}% | å›æ’¤: {info['max_dd']:.2f}")
    
    plt.rcParams['font.sans-serif'] = ['Microsoft YaHei'] # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºä¸­æ–‡æ ‡ç­¾
    plt.rcParams['axes.unicode_minus'] = False  # ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºè´Ÿå·
    # ç»˜åˆ¶ç›´æ–¹å›¾
    plt.figure(figsize=(10, 6))
    plt.hist(returns, bins=20, color='skyblue', edgecolor='black', alpha=0.7)
    plt.axvline(x=0, color='r', linestyle='--', label='ç›ˆäºå¹³è¡¡çº¿')
    plt.title('æ¨¡å‹æµ‹è¯•é›†æ”¶ç›Šåˆ†å¸ƒ (100å±€)')
    plt.xlabel('æ”¶ç›Šç‡ (ROI)')
    plt.ylabel('é¢‘æ¬¡')
    plt.legend()
    plt.grid(axis='y', alpha=0.5)
    
    plot_path = os.path.join(TRAIN_LOG_DIR, "backtest_distribution.png")
    plt.savefig(plot_path)
    print(f"ğŸ“Š æ”¶ç›Šåˆ†å¸ƒå›¾å·²ä¿å­˜è‡³: {plot_path}")
    
    # æ‰“å°ç»Ÿè®¡æ•°æ®
    returns = np.array(returns)
    print(f"\nğŸ† æœ€ç»ˆæˆç»©å•:")
    print(f"å¹³å‡æ”¶ç›Š: {np.mean(returns)*100:.2f}%")
    print(f"æ­£æ”¶ç›Šæ¯”ä¾‹: {np.sum(returns > 0)} / {len(returns)} ({np.sum(returns > 0)/len(returns)*100:.0f}%)")
    # ç®€å•çš„ç›¸å…³æ€§åˆ†æï¼šçœ‹çœ‹ Alpha é«˜çš„æ—¶å€™è¡¨ç°å¦‚ä½•
    corr = np.corrcoef(alphas, returns)[0, 1]
    print(f"Alphaä¸æ”¶ç›Šçš„ç›¸å…³æ€§: {corr:.2f} (æ­£æ•°è¡¨ç¤ºè¶Šä¿å®ˆè¶Šèµšé’±ï¼Œè´Ÿæ•°è¡¨ç¤ºè¶Šæ¿€è¿›è¶Šèµšé’±)")