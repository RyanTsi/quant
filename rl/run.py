import rl.prehandle
import numpy as np
from datetime import datetime
from rl.environment import SimpleStockEnv
from database.influx_manager import InfluxDBManager, InfluxDBConfig, InfluxDBCallbacks
from config import *
from stable_baselines3 import SAC
import os
from stable_baselines3.common.callbacks import CheckpointCallback, CallbackList, EvalCallback
from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common.utils import set_random_seed
from stable_baselines3.common.vec_env import SubprocVecEnv, VecMonitor
import pickle
import glob

class TensorboardCallback(BaseCallback):
    def __init__(self, verbose=0):
        super(TensorboardCallback, self).__init__(verbose)

    def _on_step(self) -> bool:
        # è·å–å½“å‰ step çš„ info å­—å…¸
        # self.locals['infos'] æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œå› ä¸ºå¯èƒ½æœ‰å¤šä¸ªå¹¶è¡Œç¯å¢ƒ
        info = self.locals['infos'][0]
        if "net_worth" in info:
            # å°†èµ„äº§å‡€å€¼è®°å½•åˆ° TensorBoard çš„ "Custom/NetWorth" è·¯å¾„ä¸‹
            self.logger.record("custom/net_worth", info["net_worth"])
        if "shares" in info:
            self.logger.record("custom/shares_held", info["shares"])
        if "r_base" in info:
            self.logger.record("custom/reward_base", info["r_base"])
        if "r_risk_hold" in info:
            self.logger.record("custom/reward_risk_hold", info["r_risk_hold"])
        if "r_risk_down" in info:
            self.logger.record("custom/reward_risk_down", info["r_risk_down"])
        if "r_act_pen" in info:
            self.logger.record("custom/reward_action_penalty", info["r_act_pen"])
        if "r_pos_unc" in info:
            self.logger.record("custom/reward_position_uncertainty", info["r_pos_unc"])
        if "drawdown" in info:
            self.logger.record("custom/drawdown", info["drawdown"])
        return True
    
def get_data_with_cache(manager, codes, start_date, end_date, cache_name="stock_data_cache.pkl"):
    # æ£€æŸ¥æœ¬åœ°æ˜¯å¦å­˜åœ¨ç¼“å­˜æ–‡ä»¶
    if os.path.exists(cache_name):
        print(f"ğŸ“¦ å‘ç°æœ¬åœ°ç¼“å­˜ {cache_name}ï¼Œæ­£åœ¨å¿«é€ŸåŠ è½½...")
        with open(cache_name, "rb") as f:
            return pickle.load(f)
    
    # å¦‚æœæ²¡æœ‰ç¼“å­˜ï¼Œåˆ™æ‰§è¡ŒåŸæœ‰çš„ä¸‹è½½é€»è¾‘
    print("ğŸš€ æœ¬åœ°æ— ç¼“å­˜ï¼Œå¼€å§‹ä» InfluxDB æå–æ•°æ®...")
    df_list = []
    for code in codes:
        try:
            df_temp = manager.get_stock_data_by_range(
                stock_code=code,
                start_time=start_date, 
                end_time=end_date
            )
            df_clean = rl.prehandle.preprocess_data(df_temp)
            if df_clean is not None and len(df_clean) > WINDOW_SIZE + TRAINING_DAYS:
                df_list.append(df_clean)
                print(f"âœ… {code} åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âŒ {code} åŠ è½½å¤±è´¥: {e}")
    
    # ä¸‹è½½å®Œæˆåï¼Œä¿å­˜åˆ°æœ¬åœ°
    if df_list:
        print(f"ğŸ’¾ æ­£åœ¨å°† {len(df_list)} åªè‚¡ç¥¨ä¿å­˜è‡³æœ¬åœ°ç¼“å­˜...")
        with open(cache_name, "wb") as f:
            pickle.dump(df_list, f)
            
    return df_list

SEED = 5418

def make_env(df_list, rank, seed=0):
    def _init():
        env = SimpleStockEnv(df_list)
        env.reset(seed=seed + rank)
        return env
    set_random_seed(seed)
    return _init

if __name__ == "__main__":
    # ------------ æ•°æ®å‡†å¤‡ ------------
    # 1. åˆå§‹åŒ– InfluxDB
    config = InfluxDBConfig(HOST, DATABASE, TOKEN)
    manager = InfluxDBManager(config, InfluxDBCallbacks())
    
    # 2. è·å–è‚¡ç¥¨åˆ—è¡¨å¹¶éšæœºç­›é€‰
    all_stock_codes = manager.get_stock_code_list_by_date(target_date=datetime(2025, 12, 12))
    selected_codes = np.random.choice(all_stock_codes, size=1200, replace=False)

    # 3. é€šè¿‡ç¼“å­˜è·å–æ•°æ®
    print("æ­£åœ¨åŠ è½½è®­ç»ƒé›†...")
    train_dfs = get_data_with_cache(manager, selected_codes, train_range[0], train_range[1], "train_data.pkl")
    
    print("æ­£åœ¨åŠ è½½éªŒè¯é›†...")
    val_dfs = get_data_with_cache(manager, selected_codes, val_range[0], val_range[1], "val_data.pkl")
    
    print("æ­£åœ¨åŠ è½½æµ‹è¯•é›†...")
    test_dfs = get_data_with_cache(manager, selected_codes, test_range[0], test_range[1], "test_data.pkl")

    manager.close()
    # ------------ ç¯å¢ƒæ„å»º ------------ 

    num_cpu = 20
    train_env = SubprocVecEnv([make_env(train_dfs, i, SEED) for i in range(num_cpu)])
    train_env = VecMonitor(train_env, TRAIN_LOG_DIR)

    val_env = SubprocVecEnv([make_env(val_dfs, i, SEED + 7324) for i in range(num_cpu // 2)])
    val_env = VecMonitor(val_env, VAL_LOG_DIR)
    # ------------ å›è°ƒå‡½æ•° ------------ 

    # A. éªŒè¯å›è°ƒ (EvalCallback) - æ ¸å¿ƒéƒ¨åˆ†
    # å®ƒçš„ä½œç”¨ï¼šæ¯éš” eval_freq æ­¥ï¼Œæš‚åœè®­ç»ƒï¼Œç”¨å½“å‰æ¨¡å‹åœ¨ val_env é‡Œè·‘ n_eval_episodes å±€
    # å¦‚æœå¹³å‡å¥–åŠ±åˆ›æ–°é«˜ï¼Œå°±ä¿å­˜åˆ° best_model_save_path
    eval_callback = EvalCallback(
        val_env,
        best_model_save_path='./best_model/',
        log_path=VAL_LOG_DIR,
        eval_freq=10000,        # æ¯è®­ç»ƒ 1ä¸‡æ­¥(env steps) éªŒè¯ä¸€æ¬¡
        n_eval_episodes=50,     # æ¯æ¬¡éªŒè¯è·‘ 50 å±€å–å¹³å‡ï¼Œæ¶ˆé™¤éšæœºæ€§
        deterministic=True,     # éªŒè¯æ—¶ç”±ç¡®å®šæ€§ç­–ç•¥(å»é™¤éšæœºæ¢ç´¢)ï¼Œçœ‹çœŸå®å®åŠ›
        render=False
    )

    # B. å®šæœŸä¿å­˜ (Checkpoint)
    checkpoint_callback = CheckpointCallback(save_freq=50000, save_path='./checkpoints/', name_prefix='sac_stock')
    
    # C. Tensorboard è®°å½•ç»†èŠ‚
    tb_callback = TensorboardCallback()

    # ç»„åˆå›è°ƒ
    callback_list = CallbackList([eval_callback, checkpoint_callback, tb_callback])

    # ------------ æ¨¡å‹è®­ç»ƒ ------------

    model = SAC(
        "MlpPolicy", 
        train_env, 
        tensorboard_log=TRAIN_LOG_DIR,
        learning_rate=3e-4, 
        buffer_size=1_000_000, 
        learning_starts=5000,
        batch_size=4096,
        train_freq=(100, "step"),
        gradient_steps=100,
        ent_coef='auto',
        target_entropy=-0.5,
        verbose=1,
        use_sde=True,
        device="cuda"
    )
    print("å¼€å§‹è®­ç»ƒ...")
    model.learn(
        total_timesteps=5_000_000, 
        callback=callback_list,
        reset_num_timesteps=False 
    )
    print("è®­ç»ƒç»“æŸã€‚")

    # --- æœ€ç»ˆæµ‹è¯• (Backtest) ---
    print("å¼€å§‹åœ¨æµ‹è¯•é›†ä¸Šå›æµ‹æœ€ä½³æ¨¡å‹...")
    
    # åŠ è½½éªŒè¯é›†ä¸Šè¡¨ç°æœ€å¥½çš„æ¨¡å‹
    best_model_path = os.path.join('./best_model/', "best_model.zip")
    if os.path.exists(best_model_path):
        model = SAC.load(best_model_path, device="cuda")
        print("å·²åŠ è½½æœ€ä½³æ¨¡å‹ã€‚")
    else:
        print("æœªæ‰¾åˆ°æœ€ä½³æ¨¡å‹ï¼Œä½¿ç”¨å½“å‰æœ€ç»ˆæ¨¡å‹ã€‚")

    # æ„å»ºæµ‹è¯•ç¯å¢ƒ (è¿™é‡Œå¯ä»¥ç”¨ DummyVecEnv æ–¹ä¾¿è°ƒè¯•ï¼Œæˆ–è€… SubprocVecEnv åŠ é€Ÿ)
    # æµ‹è¯•é›†æ˜¯ 2024-2025 çš„æ•°æ®
    test_env = SubprocVecEnv([make_env(test_dfs, i, SEED + 906) for i in range(num_cpu)])
    
    # è·‘æµ‹è¯•
    obs = test_env.reset()
    total_episodes = 100 # æµ‹è¯• 100 ä¸ªä¸åŒçš„è‚¡ç¥¨/æ—¶é—´æ®µ
    episode_counts = 0
    test_rewards = []
    
    # ç”¨äºè®°å½•èµ„äº§æ›²çº¿
    # æ³¨æ„ï¼šå¹¶è¡Œç¯å¢ƒå¾ˆéš¾ç”»å‡ºå•ä¸€çš„è¿ç»­æ›²çº¿ï¼Œé€šå¸¸æˆ‘ä»¬ç»Ÿè®¡åˆ†å¸ƒ
    
    while episode_counts < total_episodes:
        action, _states = model.predict(obs, deterministic=True)
        obs, rewards, dones, infos = test_env.step(action)
        
        for i, done in enumerate(dones):
            if done:
                episode_counts += 1
                # è·å–è¯¥å±€ç»“æŸæ—¶çš„ä¿¡æ¯
                if "net_worth" in infos[i]:
                     final_value = infos[i]["net_worth"]
                     roi = (final_value - ORIGINAL_MONEY) / ORIGINAL_MONEY
                     test_rewards.append(roi)
                     print(f"æµ‹è¯•å±€ {episode_counts}: æ”¶ç›Šç‡ {roi*100:.2f}%")

    print(f"å¹³å‡æµ‹è¯•æ”¶ç›Šç‡: {np.mean(test_rewards)*100:.2f}%")
    print(f"æ­£æ”¶ç›Šæ¯”ä¾‹: {np.sum(np.array(test_rewards) > 0) / len(test_rewards) * 100:.2f}%")
    
    test_env.close()
    train_env.close()
    val_env.close()