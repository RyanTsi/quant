æˆ‘æ­£åœ¨ä½¿ç”¨RLè®­ç»ƒé‡åŒ–æ¨¡å‹ï¼Œè¿™ä¸ªæ¨¡å‹æ˜¯ä¸€ä¸ªæŒ‡ç¤ºå™¨çš„ä½œç”¨æ¥ä½œä¸ºä¸‹ä¸€ä¸ªæ¨¡å‹çš„ä¸€ç»´è¾“å…¥ï¼Œæˆ–è€…æ˜¯æˆ‘ä¹°å…¥å–å‡ºè‚¡ç¥¨çš„æç¤ºï¼Œæˆ‘ç§°å…¶ä¸ºæ”¹è‰¯alphaã€‚å®ƒä»¥Tçš„å°¾ç›˜ä»·æ ¼ä½œä¸ºè¾“å…¥ï¼Œé¢„æµ‹T+1çš„æ”¶ç›˜ä»·å¯¹æ¯”Tçš„æ”¶ç›˜ä»·æ˜¯æ¶¨äº†è¿˜æ˜¯è·Œäº†ã€‚

ä¸‹é¢æ˜¯æˆ‘çš„è®­ç»ƒç¯å¢ƒçš„ä»£ç 
```python
import numpy as np
import gymnasium as gym
from gymnasium import spaces

# ==========================================
# 1. æ ¸å¿ƒç¯å¢ƒç±»: AStockSignalEnv v4.0
# ==========================================
class AStockSignalEnv(gym.Env):
    """
    Aè‚¡ä¿¡å·äº¤æ˜“ç¯å¢ƒ v4.0 (Production Ready)
    ç‰¹æ€§:
    - åŠ¨æ€ Rolling Z-Score å½’ä¸€åŒ– (æ— æœªæ¥å‡½æ•°)
    - ä¸¥æ ¼çš„æµåŠ¨æ€§æ£€æŸ¥ (åœç‰Œ/ä¸€å­—æ¿æ— æ³•äº¤æ˜“)
    - äº¤æ˜“æ­»åŒº (è¿‡æ»¤å¾®å°éœ‡è¡)
    - çœŸå®å‡€å€¼è¿½è¸ª (Portfolio Value)
    """
    def __init__(self, stock_df_list: list,
                 window_size=60,
                 training_days=252,
                 transaction_cost_pct=0.0010,  # å•è¾¹ä¸‡åˆ†ä¹‹10 (å«å°èŠ±ç¨+ä½£é‡‘+æ»‘ç‚¹)
                 deadzone_level=0.1,           # 10% ä»“ä½å˜åŒ–æ­»åŒº
                 reward_scale=0.1):            # Reward ç¼©æ”¾å› å­
        super(AStockSignalEnv, self).__init__()
        
        self.window_size = window_size
        self.training_days = training_days
        self.transaction_cost_pct = transaction_cost_pct
        self.deadzone_level = deadzone_level
        self.reward_scale = reward_scale
        
        print(f"æ­£åœ¨åˆå§‹åŒ–ç¯å¢ƒ (v4.0)...")
        self.data_list = []      # (N_stocks, T, Features)
        self.target_list = []    # (N_stocks, T)
        self.abs_ret_list = []  # (N_stocks, T)
        
        # list[0] æ˜¯å¤§ç›˜æŒ‡æ•°
        if len(stock_df_list) < 2:
            raise ValueError("éœ€è¦è‡³å°‘ä¸¤ä¸ªDataFrame: [0]ä¸ºæŒ‡æ•°, [1:]ä¸ºä¸ªè‚¡")
            
        index_df = stock_df_list[0]
        
        # æ‰¹é‡é¢„å¤„ç†
        print("å¼€å§‹ç‰¹å¾å·¥ç¨‹ (Rolling Window)...")
        for i, df in enumerate(stock_df_list[1:]):
            feats, targs, abs_ret = self._preprocess_data(df, index_df)
            
            # ç¡®ä¿æ•°æ®é•¿åº¦è¶³å¤Ÿ
            # éœ€è¦: Window + Training Steps + Buffer
            if len(feats) > window_size + training_days + 20:
                self.data_list.append(feats)
                self.target_list.append(targs)
                self.abs_ret_list.append(abs_ret)
            
            if i % 500 == 0 and i > 0:
                print(f"å·²å¤„ç† {i} åªè‚¡ç¥¨...")
                
        print(f"åˆå§‹åŒ–å®Œæˆï¼Œæœ‰æ•ˆè‚¡ç¥¨æ•°é‡: {len(self.data_list)}")

        # åŠ¨ä½œç©ºé—´: [-1, 1]
        self.action_space = spaces.Box(low=-1, high=1, shape=(1,), dtype=np.float32)
        
        # è§‚æµ‹ç©ºé—´: (Window, 5ç‰¹å¾)
        # ç‰¹å¾: [Log_Ret, Rel_Str, MA_Bias, Vol_Ratio, Hist_Vol]
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, 
            shape=(window_size, 5), dtype=np.float32
        )
        
        # è¿è¡Œæ—¶å†…éƒ¨çŠ¶æ€
        self.current_stock_idx = 0
        self.day_idx = 0
        self.steps_taken = 0
        self.last_signal = 0.0
        self.portfolio_value = 1.0

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        
        # 1. éšæœºé€‰ä¸€åªè‚¡ç¥¨
        self.current_stock_idx = np.random.randint(0, len(self.data_list))
        data_len = len(self.data_list[self.current_stock_idx])
        
        # 2. éšæœºé€‰ä¸€ä¸ªæ—¶é—´èµ·ç‚¹
        # å¿…é¡»ç•™å‡º training_days çš„ä½™é‡
        max_start = data_len - self.training_days - 1
        start_idx_min = self.window_size
        
        if max_start <= start_idx_min:
            self.day_idx = start_idx_min
        else:
            self.day_idx = np.random.randint(start_idx_min, max_start + 1)
            
        # 3. é‡ç½®çŠ¶æ€
        self.steps_taken = 0
        self.last_signal = 0.0
        self.portfolio_value = 1.0
        
        return self._get_observation(), {}

    def step(self, action):
        # -----------------------------------------------------------
        # A. è§£æåŠ¨ä½œä¸çŠ¶æ€
        # -----------------------------------------------------------
        # 1. æˆªæ–­ Action åˆ° [-1, 1]
        signal = float(np.clip(action[0], -1, 1))
        
        # 2. è®¡ç®—ä»“ä½å˜åŒ–é‡ (ç”¨äºè®¡ç®—æ‰‹ç»­è´¹)
        effective_change = np.abs(signal - self.last_signal)

        # -----------------------------------------------------------
        # B. è·å–ç¯å¢ƒæ•°æ® (Ground Truth)
        # -----------------------------------------------------------
        # è¾¹ç•Œæ£€æŸ¥
        current_alpha_targets = self.target_list[self.current_stock_idx]
        if self.day_idx >= len(current_alpha_targets) - 1:
            return np.zeros(self.observation_space.shape, dtype=np.float32), 0.0, True, False, {}

        # [å…³é”®åŒºåˆ«]
        # actual_alpha_pct: è¶…é¢æ”¶ç›Š (Stock - Index)ã€‚è¿™æ˜¯æ¨¡å‹è¦å­¦ä¹ çš„ç›®æ ‡ã€‚
        # actual_abs_pct:   ç»å¯¹æ”¶ç›Š (Stock)ã€‚è¿™æ˜¯è´¦æˆ·çœŸå®çš„ç›ˆäºã€‚
        actual_alpha_pct = current_alpha_targets[self.day_idx]
        actual_abs_pct = self.abs_ret_list[self.current_stock_idx][self.day_idx]
        
        # åæ¨æŒ‡æ•°æ”¶ç›Š (ç”¨äºè°ƒè¯•: Alpha + Index ~= Abs)
        # æ³¨æ„: ç”±äºæµ®ç‚¹è¯¯å·®å’ŒLog Returnè®¡ç®—æ–¹å¼ï¼Œè¿™é‡Œåªæ˜¯è¿‘ä¼¼ï¼Œä½†è¶³å¤Ÿè°ƒè¯•ç”¨
        actual_index_pct = actual_abs_pct - actual_alpha_pct

        # -----------------------------------------------------------
        # C. è®¡ç®— Reward (æŒ‡å¯¼æ¨¡å‹å­¦ä¹ çš„ä¿¡å·)
        # -----------------------------------------------------------
        # 1. åŸå§‹ Alpha æ”¶ç›Š (Raw Alpha Reward)
        # å¦‚æœ Signal > 0 ä¸” Alpha > 0 (è·‘èµ¢å¤§ç›˜)ï¼Œæ­£å¥–åŠ±
        # å¦‚æœ Signal > 0 ä¸” Alpha < 0 (è·‘è¾“å¤§ç›˜)ï¼Œè´Ÿå¥–åŠ±
        raw_reward = signal * actual_alpha_pct
        
        # 2. äº¤æ˜“æˆæœ¬ (Cost)
        # å»ºè®®: è®­ç»ƒåˆæœŸ transaction_cost_pct è®¾æä½ (å¦‚ 0 æˆ– 1e-5)ï¼Œè®©æ¨¡å‹å…ˆæ•¢äºäº¤æ˜“
        cost_pct = effective_change * self.transaction_cost_pct * 100
        
        # 3. æœ€ç»ˆ Reward
        # åªæœ‰æ‰£é™¤æˆæœ¬åè¿˜èƒ½è·‘èµ¢å¤§ç›˜ï¼Œæ‰æ˜¯å¥½çš„ Alpha
        reward = (raw_reward - cost_pct) * self.reward_scale

        # -----------------------------------------------------------
        # D. å‡€å€¼è¿½è¸ª (Portfolio Value - çœŸå®çš„é’±)
        # -----------------------------------------------------------
        # è¿™é‡Œçš„ç›ˆäºå¿…é¡»ç”¨ "ç»å¯¹æ”¶ç›Š" ç®—ï¼Œå› ä¸ºä½ åœ¨å®ç›˜é‡Œä¸èƒ½åªä¹° Alpha
        gross_abs_ret = signal * actual_abs_pct
        net_abs_ret = gross_abs_ret - cost_pct
        
        self.portfolio_value *= (1 + net_abs_ret / 100.0)

        # -----------------------------------------------------------
        # E. è®¡ç®—è°ƒè¯•æŒ‡æ ‡ (Info Engineering)
        # -----------------------------------------------------------
        # 1. èƒœç‡åˆ¤å®š: åªæœ‰å½“æ–¹å‘æ­£ç¡®ä¸”äº§ç”Ÿæ­£å‘ Alpha æ—¶æ‰ç®— Win
        # é¿å… 0 å€¼å™ªéŸ³ï¼Œè®¾ä¸€ä¸ªæå°é˜ˆå€¼
        is_win = 1.0 if (signal * actual_alpha_pct > 1e-5) else 0.0
        
        # 2. ä¿¡å·ç½®ä¿¡åº¦ (æ¨¡å‹å¼€ä»“æœ‰å¤šé‡?)
        confidence = np.abs(signal)

        # -----------------------------------------------------------
        # F. çŠ¶æ€æ›´æ–°ä¸è¾“å‡º
        # -----------------------------------------------------------
        self.last_signal = signal
        self.day_idx += 1
        self.steps_taken += 1
        
        truncated = self.steps_taken >= self.training_days
        terminated = False # é™¤éç ´äº§ï¼Œå¦åˆ™ä¸è‡ªè¡Œç»ˆæ­¢ï¼Œè®© TimeLimit å¤„ç†
        
        # è¿™ä¸€æ­¥ä¸€å®šè¦ä¿è¯ preprocess é‡Œçš„ dropna æ˜¯æ‰§è¡Œè¿‡çš„
        next_obs = self._get_observation()
        
        info = {
            # === 1. æ ¸å¿ƒè¡¨ç° (è®­ç»ƒç›‘æ§) ===
            'Reward': reward,                          # æœ€ç»ˆç»™ RL çš„åˆ†
            'Metrics/Raw_Alpha_Ret': raw_reward,       # æœªæ‰£è´¹çš„ Alpha æ”¶ç›Š
            'Metrics/Cost': cost_pct,                  # æ‰‹ç»­è´¹æŸè€—
            
            # === 2. å½’å› åˆ†æ (æœ€é‡è¦ - åŒºåˆ†è¿æ°”å’Œå®åŠ›) ===
            # å¦‚æœ Portfolio æ¶¨äº†ï¼Œæ˜¯å› ä¸º Alpha (å®åŠ›) è¿˜æ˜¯ Index (è¿æ°”)?
            'Attribution/Alpha_Ret_Day': actual_alpha_pct, # å½“å¤©è¯¥è‚¡è¶…é¢æ”¶ç›Š
            'Attribution/Index_Ret_Day': actual_index_pct, # å½“å¤©å¤§ç›˜æ”¶ç›Š
            'Attribution/Abs_Ret_Day': actual_abs_pct,     # å½“å¤©ä¸ªè‚¡ç»å¯¹æ”¶ç›Š
            
            # === 3. æ¨¡å‹è¡Œä¸ºè¯Šæ–­ ===
            'Action/Signal': signal,                   # ä¿¡å·å€¼ (-1 ~ 1)
            'Action/Confidence': confidence,           # å¼€ä»“åŠ›åº¦ (0 ~ 1)
            'Metrics/Win_Rate_Step': is_win,           # å•æ­¥èƒœç‡ (0 æˆ– 1, Tensorboard ä¼šè‡ªåŠ¨ç®—å¹³å‡)
            
            # === 4. è´¦æˆ·çŠ¶æ€ ===
            'State/Portfolio_Value': self.portfolio_value
        }

        return next_obs, reward, terminated, truncated, info

    def _get_observation(self):
        # åˆ‡ç‰‡è·å–çª—å£æ•°æ®: [T - Window + 1 : T + 1]
        start = self.day_idx - self.window_size + 1
        end = self.day_idx + 1
        
        raw_obs = self.data_list[self.current_stock_idx][start:end]
        
        # æ³¨å…¥å¾®å°å™ªå£°ï¼Œå¢åŠ é²æ£’æ€§
        # å› ä¸ºç‰¹å¾å·²ç»æ˜¯ Z-Score (~N(0,1))ï¼Œå™ªå£°ç»™ 0.01 è¶³å¤Ÿ
        noise = np.random.normal(0, 0.01, size=raw_obs.shape)
        return (raw_obs + noise).astype(np.float32)

    def _preprocess_data(self, df, index_df):
        """
        ä¸¥è°¨çš„ç‰¹å¾å·¥ç¨‹:
        """
        df = df.copy()
        if 'time' in df.columns:
            # ç¡®ä¿è½¬ä¸º datetime æ ¼å¼
            df['time'] = pd.to_datetime(df['time'])
            # è®¾ç½®ä¸ºç´¢å¼•
            df.set_index('time', inplace=True)
            # æ’åºï¼ˆä»¥é˜²ä¸‡ä¸€ï¼‰
            df.sort_index(inplace=True)
        # 1. æ•°æ®å¯¹é½
        df = df.join(index_df[['æ”¶ç›˜']], rsuffix='_Idx', how='inner')
        
        # 2. è®¡ç®—åŸºç¡€ Log Return
        # log(P_t / P_{t-1}) * 100
        df['Log_Ret'] = np.log(df['æ”¶ç›˜'] / df['æ”¶ç›˜'].shift(1)) * 100
        df['Index_Log_Ret'] = np.log(df['æ”¶ç›˜_Idx'] / df['æ”¶ç›˜_Idx'].shift(1)) * 100
        df['Excess_Ret'] = df['Log_Ret'] - df['Index_Log_Ret']
        # 3. æ„å»ºåŸå§‹ç‰¹å¾
        # F1: ç›¸å¯¹å¼ºåº¦
        df['Rel_Str'] = df['Log_Ret'] - df['Index_Log_Ret']
        
        # F2: å‡çº¿ä¹–ç¦»
        ma_20 = df['æ”¶ç›˜'].rolling(20).mean()
        df['MA_Bias'] = (df['æ”¶ç›˜'] - ma_20) / (ma_20 + 1e-8) * 100
        
        # F3: é‡æ¯” (Log)
        vol_ma_5 = df['æˆäº¤é¢'].rolling(5).mean()
        df['Vol_Ratio'] = np.log(df['æˆäº¤é¢'] / (vol_ma_5.shift(1) + 1e-8))
        
        # F4: å†å²æ³¢åŠ¨ç‡
        df['Hist_Vol'] = df['Log_Ret'].rolling(20).std()
        
        # 4. === åŠ¨æ€å½’ä¸€åŒ– (Rolling Z-Score) ===
        feature_cols = ['Log_Ret', 'Rel_Str', 'MA_Bias', 'Vol_Ratio', 'Hist_Vol']
        roll_window = 60 # ç»Ÿè®¡çª—å£
        
        for col in feature_cols:
            # è®¡ç®—æ»šåŠ¨ç»Ÿè®¡é‡
            roll = df[col].rolling(window=roll_window, min_periods=20)
            roll_mean = roll.mean()
            roll_std = roll.std()
            
            # å½’ä¸€åŒ–: (x - mean) / std
            df[col] = (df[col] - roll_mean) / (roll_std + 1e-8)
            # æˆªæ–­æç«¯å€¼
            df[col] = df[col].clip(-5, 5)
            
        # 5. å‡†å¤‡ Target (T+1 æ”¶ç›Š)
        df['Next_Excess_Ret'] = df['Excess_Ret'].shift(-1)
        df['Next_Abs_Ret'] = df['Log_Ret'].shift(-1)
        # 6. æ¸…æ´— NaN (Rollingå¯¼è‡´çš„å¤´éƒ¨ç¼ºå¤± + Shiftå¯¼è‡´çš„å°¾éƒ¨ç¼ºå¤±)
        df.dropna(inplace=True)
        
        return (
            df[feature_cols].values.astype(np.float32),
            df['Next_Excess_Ret'].values.astype(np.float32),
            df['Next_Abs_Ret'].values.astype(np.float32),
        )
```

ä¸‹é¢æ˜¯æˆ‘çš„è®­ç»ƒçš„ä»£ç 
```python
import random
import numpy as np
import os
import pickle
import matplotlib.pyplot as plt
from datetime import datetime
import pandas as pd
from stable_baselines3 import SAC
from stable_baselines3.common.callbacks import CheckpointCallback, CallbackList, EvalCallback, BaseCallback
from stable_baselines3.common.utils import set_random_seed
from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
import gymnasium as gym
import torch
import torch.nn as nn

# --- è‡ªå®šä¹‰æ¨¡å—å¯¼å…¥ ---
import rl.prehandle
from rl.signalEnv import AStockSignalEnv
from database.influx_manager import InfluxDBManager, InfluxDBConfig, InfluxDBCallbacks
from config import * 

# ==========================================
# 1. æ—¥å¿—å›è°ƒ
# ==========================================
class DetailedLogCallback(BaseCallback):
    """
    ä» Env çš„ info ä¸­æå–è‡ªå®šä¹‰æŒ‡æ ‡å¹¶è®°å½•åˆ° TensorBoard
    """
    def _on_step(self) -> bool:
        # 1. è·å–å½“å‰ Step æ‰€æœ‰ç¯å¢ƒè¿”å›çš„ info (åˆ—è¡¨ï¼Œé•¿åº¦ä¸ºç¯å¢ƒæ•°é‡)
        infos = self.locals.get('infos', [])
        
        # 2. éå†ç¯å¢ƒ (é€šå¸¸ä½ ç”¨ DummyVecEnv åªæœ‰ä¸€ä¸ªç¯å¢ƒï¼Œä½†ä¸ºäº†é€šç”¨æ€§è¿™é‡Œç”¨å¾ªç¯)
        for info in infos:
            
            # --- A. è´¦æˆ·çŠ¶æ€ (æœ€æ ¸å¿ƒ) ---
            if 'State/Portfolio_Value' in info:
                # è®°å½•å‡€å€¼æ›²çº¿
                self.logger.record("main/Portfolio_Value", info['State/Portfolio_Value'])

            # --- B. è®­ç»ƒç›‘æ§ (Metrics) ---
            if 'Metrics/Raw_Alpha_Ret' in info:
                # åŸå§‹ Alpha æ”¶ç›Š (æœªæ‰£è´¹)
                self.logger.record("train/Raw_Alpha_Ret", info['Metrics/Raw_Alpha_Ret'])
            
            if 'Metrics/Cost' in info:
                # äº¤æ˜“æˆæœ¬æŸè€—
                self.logger.record("train/Transaction_Cost", info['Metrics/Cost'])
                
            if 'Metrics/Win_Rate_Step' in info:
                # èƒœç‡ (SB3 ä¼šè‡ªåŠ¨è®¡ç®— dump é—´éš”å†…çš„å¹³å‡å€¼)
                self.logger.record("train/Win_Rate", info['Metrics/Win_Rate_Step'])

            # --- C. å½’å› åˆ†æ (Attribution) ---
            # è¿™é‡Œçš„ç›®çš„æ˜¯çœ‹ï¼šä½ çš„æ”¶ç›Šåˆ°åº•æ¥è‡ªäº Alpha è¿˜æ˜¯å¤§ç›˜ Beta
            if 'Attribution/Alpha_Ret_Day' in info:
                self.logger.record("attribution/Alpha_Ret", info['Attribution/Alpha_Ret_Day'])
            
            if 'Attribution/Index_Ret_Day' in info:
                self.logger.record("attribution/Index_Ret", info['Attribution/Index_Ret_Day'])
                
            if 'Attribution/Abs_Ret_Day' in info:
                self.logger.record("attribution/Abs_Ret", info['Attribution/Abs_Ret_Day'])

            # --- D. è¡Œä¸ºè¯Šæ–­ (Behavior) ---
            # è§‚å¯Ÿæ¨¡å‹æ˜¯ä¸æ˜¯åªä¼šè¾“å‡º 0ï¼Œæˆ–è€…ç–¯ç‹‚è¾“å‡º 1/-1
            if 'Action/Signal' in info:
                self.logger.record("behavior/Signal_Mean", info['Action/Signal'])
                
            if 'Action/Confidence' in info:
                self.logger.record("behavior/Confidence", info['Action/Confidence'])

        return True

# ==========================================
# 2. æ•°æ®åŠ è½½å·¥å…· (å·²ä¿®æ”¹ï¼šç§»é™¤å¤–éƒ¨é¢„å¤„ç†)
# ==========================================
def get_data_with_cache(manager, codes, start_date, end_date, cache_name):
    """
    ä¿®æ”¹åçš„æ•°æ®åŠ è½½é€»è¾‘ï¼š
    1. ä»æ•°æ®åº“æ‹‰å–åŸå§‹æ•°æ®
    2. ä½¿ç”¨ rl.prehandle.preprocess_data è¿›è¡Œæ¸…æ´— (å‰”é™¤STã€æ­»è‚¡)
    3. åªæœ‰æ¸…æ´—åˆæ ¼çš„æ•°æ®æ‰è¿›å…¥åˆ—è¡¨
    """
    if os.path.exists(cache_name):
        print(f"ğŸ“¦ å‘ç°ç¼“å­˜ {cache_name}ï¼Œå¿«é€ŸåŠ è½½ä¸­...")
        with open(cache_name, "rb") as f:
            return pickle.load(f)
    
    print(f"ğŸš€ æœ¬åœ°æ— ç¼“å­˜ï¼Œå¼€å§‹ä¸‹è½½åŠæ¸…æ´— {len(codes)} åªè‚¡ç¥¨æ•°æ®...")
    df_list = []
    
    # å¿…é¡»ä¿è¯ index=0 æ˜¯å¤§ç›˜æŒ‡æ•°
    # æˆ‘ä»¬å‡è®¾ codes[0] æ˜¯ sh000001
    
    # å…ˆå¤„ç†å¤§ç›˜æŒ‡æ•° (æŒ‡æ•°ä¸éœ€è¦ç»è¿‡ ST è¿‡æ»¤ï¼Œä½†ä¹Ÿéœ€è¦åŸºç¡€æ¸…æ´—)
    try:
        index_df = manager.get_stock_data_by_range(stock_code=codes[0], start_time=start_date, end_time=end_date)
        # æŒ‡æ•°åªéœ€è¦åŸºç¡€æ¸…æ´—ï¼ˆå»ç©ºã€æ’åºï¼‰
        if index_df is not None:
             # ç¡®ä¿æ—¶é—´æ’åº
            if 'time' in index_df.columns:
                index_df['time'] = pd.to_datetime(index_df['time'])
                index_df = index_df.sort_values('time').set_index('time')
            elif not isinstance(index_df.index, pd.DatetimeIndex):
                 pass 
            
            # æŒ‡æ•°å¿…é¡»ä¿ç•™
            df_list.append(index_df)
            print(f"âœ… æŒ‡æ•°æ•°æ®å·²åŠ è½½: {len(index_df)} æ¡")
    except Exception as e:
        print(f"âŒ æŒ‡æ•°è·å–å¤±è´¥: {e}")
        return [] # æŒ‡æ•°æŒ‚äº†å°±æ²¡æ³•ç»ƒäº†

    # å¤„ç†ä¸ªè‚¡
    valid_count = 0
    skipped_count = 0
    
    for code in codes[1:]: # è·³è¿‡ç¬¬ä¸€ä¸ªï¼ˆå› ä¸ºæ˜¯æŒ‡æ•°ï¼‰
        try:
            df_temp = manager.get_stock_data_by_range(stock_code=code, start_time=start_date, end_time=end_date)
            
            # === è°ƒç”¨æ‚¨çš„æ¸…æ´—é€»è¾‘ ===
            # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¼ å…¥äº† codeï¼Œç”¨äºå‰ç¼€åˆ¤æ–­
            df_clean = rl.prehandle.preprocess_data(df_temp)
            
            if df_clean is not None:
                df_list.append(df_clean)
                valid_count += 1
            else:
                skipped_count += 1
                
        except Exception as e:
            print(f"âŒ {code} å¤„ç†å¼‚å¸¸: {e}")
            skipped_count += 1
            
        # è¿›åº¦æ‰“å°
        if (valid_count + skipped_count) % 500 == 0:
            print(f"å¤„ç†è¿›åº¦: æœ‰æ•ˆ {valid_count} / è·³è¿‡ {skipped_count} ...")
    
    print(f"ğŸ“Š æ•°æ®æ¸…æ´—å®Œæˆ: è¾“å…¥ {len(codes)-1} -> è¾“å‡º {valid_count} (å‰”é™¤ç‡ {skipped_count/(len(codes)-1):.1%})")

    if len(df_list) > 1: # è‡³å°‘è¦æœ‰ 1ä¸ªæŒ‡æ•° + 1ä¸ªè‚¡ç¥¨
        print(f"ğŸ’¾ ä¿å­˜ç¼“å­˜è‡³ {cache_name}...")
        with open(cache_name, "wb") as f:
            pickle.dump(df_list, f)
            
    return df_list

class LSTM_Attention_Extractor(BaseFeaturesExtractor):
    """
    å·¥ä¸šçº§æ—¶åºç‰¹å¾æå–å™¨
    ç»“æ„: Input -> LSTM -> (Attention) -> Linear -> Output to Policy
    """
    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 256):
        # åˆå§‹åŒ–çˆ¶ç±»ï¼Œfeatures_dim æ˜¯è¾“å‡ºç»™ SAC Actor/Critic çš„å‘é‡é•¿åº¦
        super().__init__(observation_space, features_dim)
        
        # 1. è‡ªåŠ¨æ¨æ–­è¾“å…¥ç»´åº¦
        # observation_space.shape é€šå¸¸æ˜¯ (Window_Size, Feature_Num)
        # ä¾‹å¦‚ (60, 5)
        self.window_size = observation_space.shape[0]
        self.input_features = observation_space.shape[1]
        
        # 2. å®šä¹‰ LSTM å±‚
        # hidden_size: éšå±‚ç»´åº¦ï¼Œè¶Šå¤§æ‹Ÿåˆèƒ½åŠ›è¶Šå¼ºï¼Œä½†è¶Šéš¾è®­ç»ƒ
        hidden_size = 64
        self.lstm = nn.LSTM(
            input_size=self.input_features,
            hidden_size=hidden_size,
            num_layers=2,           # å †å ä¸¤å±‚ LSTM æå–æ·±å±‚ç‰¹å¾
            batch_first=True,       # è¾“å…¥æ ¼å¼ (Batch, Seq, Feature)
            dropout=0.2             # é˜²æ­¢è¿‡æ‹Ÿåˆ
        )
        
        # 3. (å¯é€‰) ç®€å•çš„æ³¨æ„åŠ›æœºåˆ¶å±‚
        # ç”¨äºè®¡ç®— LSTM è¾“å‡ºåºåˆ—ä¸­æ¯ä¸ªæ—¶é—´æ­¥çš„æƒé‡
        self.attention = nn.Sequential(
            nn.Linear(hidden_size, 32),
            nn.Tanh(),
            nn.Linear(32, 1),
            nn.Softmax(dim=1)
        )
        
        # 4. æœ€ç»ˆæ˜ å°„å±‚
        # å°† LSTM/Attention çš„è¾“å‡ºæ˜ å°„åˆ° features_dim (256)
        self.linear = nn.Sequential(
            nn.Linear(hidden_size, features_dim),
            nn.LayerNorm(features_dim), # LayerNorm å¯¹é‡‘èæ—¶åºéå¸¸é‡è¦ï¼Œç¨³å®šæ¢¯åº¦
            nn.ReLU()
        )

    def forward(self, observations: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­é€»è¾‘
        observations shape: (Batch_Size, Window_Size, Features)
        """
        # 1. ç¡®ä¿è¾“å…¥æ˜¯ Float ç±»å‹
        x = observations.float()
        
        # 2. LSTM å‰å‘ä¼ æ’­
        # out: (Batch, Window, Hidden)
        # (h_n, c_n): æœ€åæ—¶åˆ»çš„éšçŠ¶æ€
        lstm_out, (h_n, c_n) = self.lstm(x)
        
        # --- ç­–ç•¥ A: ä»…ä½¿ç”¨æœ€åä¸€æ­¥ (ç»å…¸åšæ³•) ---
        # feature_vector = lstm_out[:, -1, :] 
        
        # --- ç­–ç•¥ B: ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶ (è¿›é˜¶åšæ³• - æ¨è) ---
        # è®¡ç®—æƒé‡: (Batch, Window, 1)
        weights = self.attention(lstm_out) 
        # åŠ æƒæ±‚å’Œ: (Batch, Hidden)
        # è¿™é‡Œçš„å«ä¹‰æ˜¯ï¼šæ¨¡å‹è‡ªåŠ¨å­¦ä¼šè¿™60å¤©é‡Œï¼Œå“ªå‡ å¤©å¯¹é¢„æµ‹T+1æœ€é‡è¦
        context_vector = torch.sum(weights * lstm_out, dim=1)
        
        # 3. æœ€ç»ˆæ˜ å°„
        return self.linear(context_vector)
    

# ==========================================
# 3. ä¸»ç¨‹åº
# ==========================================
SEED = 541438
ADDITIONAL_STEPS = 2_000_000 

if __name__ == "__main__":
    set_random_seed(SEED)
    
    # --- A. æ•°æ®å‡†å¤‡ ---
    # ç¡®ä¿ config.py ä¸­å®šä¹‰äº† HOST, DATABASE, TOKEN ç­‰
    config_db = InfluxDBConfig(HOST, DATABASE, TOKEN)
    manager = InfluxDBManager(config_db, InfluxDBCallbacks())
    
    # è·å–è‚¡ç¥¨åˆ—è¡¨
    target_date = datetime(2023, 12, 12)
    all_codes = manager.get_stock_code_list_by_date(target_date)
    
    # ç¡®ä¿å¤§ç›˜æŒ‡æ•°åœ¨ç¬¬ä¸€ä½
    index_code = "sh000001"
    if index_code in all_codes:
        all_codes.remove(index_code)
    all_codes.insert(0, index_code)

    # åŠ è½½æ•°æ® (Train/Val/Test)
    # è¿™é‡Œçš„ train_range ç­‰å˜é‡éœ€åœ¨ config.py ä¸­å®šä¹‰
    print("æ­£åœ¨åŠ è½½è®­ç»ƒé›†...")
    train_dfs = get_data_with_cache(manager, all_codes, train_range[0], train_range[1], "train_data_v4.pkl")
    print("æ­£åœ¨åŠ è½½éªŒè¯é›†...")
    val_dfs   = get_data_with_cache(manager, all_codes, val_range[0], val_range[1], "val_data_v4.pkl")
    
    manager.close()

    # --- B. ç¯å¢ƒæ„å»º ---
    # æ³¨æ„ï¼šä½¿ç”¨ v4.0 çš„å‚æ•°é…ç½®
    env_kwargs = {
        'window_size': 60,
        'training_days': 252,
        'transaction_cost_pct': 0.0000,
        'deadzone_level': 0.1,
        'reward_scale': 1
    }

    print("æ„å»ºè®­ç»ƒç¯å¢ƒ...")
    train_env = DummyVecEnv([lambda: AStockSignalEnv(train_dfs, **env_kwargs)])
    train_env = VecMonitor(train_env, TRAIN_LOG_DIR)

    print("æ„å»ºéªŒè¯ç¯å¢ƒ...")
    val_env = DummyVecEnv([lambda: AStockSignalEnv(val_dfs, **env_kwargs)])
    val_env = VecMonitor(val_env, VAL_LOG_DIR)

    # --- C. å›è°ƒå‡½æ•°ç»„è£… ---
    
    # 1. éªŒè¯å›è°ƒ
    eval_callback = EvalCallback(
        val_env,
        best_model_save_path='./best_modelV4/',
        log_path=VAL_LOG_DIR,
        eval_freq=5_000,        # ç¨å¾®é™ä½é¢‘ç‡ï¼ŒåŠ å¿«è®­ç»ƒé€Ÿåº¦
        n_eval_episodes=20,     # éªŒè¯20ä¸ªEpisode (20åªéšæœºè‚¡ç¥¨/æ—¶é—´æ®µ)
        deterministic=True,
        render=False
    )
    
    # 2. æ£€æŸ¥ç‚¹å›è°ƒ
    checkpoint_callback = CheckpointCallback(
        save_freq=5_000, 
        save_path='./checkpoints_v4/', 
        name_prefix='sac_v4'
    )
    
    # 3. è¯¦ç»†æ—¥å¿—å›è°ƒ
    log_callback = DetailedLogCallback()

    callback_list = CallbackList([eval_callback, checkpoint_callback, log_callback])

    # --- D. æ¨¡å‹åŠ è½½ä¸è®­ç»ƒ ---
    best_model_path = "./best_modelV4/best_model.zip"
    
    if os.path.exists(best_model_path):
        print(f"ğŸ”„ å‘ç°ç°æœ‰æ¨¡å‹ {best_model_path}ï¼Œæ­£åœ¨åŠ è½½...")
        model = SAC.load(best_model_path, env=train_env, device="cuda")
        
        # å°è¯•åŠ è½½ Replay Buffer
        buffer_path = "./best_modelV4/replay_buffer.pkl"
        if os.path.exists(buffer_path):
            try:
                print("ğŸ’¾ åŠ è½½ Replay Buffer...")
                model.load_replay_buffer(buffer_path)
            except Exception as e:
                print(f"âš ï¸ Buffer åŠ è½½å¤±è´¥ (å¯èƒ½æ˜¯Obs Shapeå˜äº†): {e}")
                
        print(f"ğŸ“ˆ ç»§ç»­è®­ç»ƒï¼Œç›®æ ‡æ­¥æ•°: {ADDITIONAL_STEPS}")
        model.learn(total_timesteps=ADDITIONAL_STEPS, callback=callback_list, reset_num_timesteps=False)
        
    else:
        print("ğŸ†• åˆ›å»ºå…¨æ–° SAC æ¨¡å‹ (V4 Environment)...")
        policy_kwargs = dict(
            # 1. æŒ‡å®šè‡ªå®šä¹‰æå–å™¨
            features_extractor_class=LSTM_Attention_Extractor,
            
            # 2. ä¼ é€’å‚æ•°ç»™æå–å™¨ (å¯¹åº” __init__ ä¸­çš„å‚æ•°)
            features_extractor_kwargs=dict(features_dim=256),
            
            # 3. å®šä¹‰æå–å™¨ä¹‹åçš„ç½‘ç»œç»“æ„ (Actor å’Œ Critic)
            # å› ä¸º LSTM å·²ç»æå–äº†å¼ºåŠ›çš„ç‰¹å¾ï¼Œåé¢çš„ç½‘ç»œå¯ä»¥ç¨å¾®ç®€å•ç‚¹
            net_arch=dict(pi=[128, 64], qf=[128, 64]),
            
            # 4. ä¼˜åŒ–å™¨å‚æ•° (å¯é€‰ï¼Œå¾®è°ƒ)
            optimizer_kwargs=dict(weight_decay=1e-5) # L2 æ­£åˆ™åŒ–ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
        )
        # é’ˆå¯¹é‡‘èæ—¶é—´åºåˆ—è°ƒæ•´çš„ SAC å‚æ•°
        model = SAC(
            "MlpPolicy", 
            train_env, 
            verbose=1, 
            tensorboard_log=TRAIN_LOG_DIR,
            device="cuda",
            buffer_size=500_000,
            learning_starts=20_000,
            batch_size=512,
            ent_coef='auto',
            # policy_kwargs=dict(net_arch=[256, 256])
            policy_kwargs=policy_kwargs
        )
        
        print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
        model.learn(total_timesteps=ADDITIONAL_STEPS, callback=callback_list)

    # --- E. ä¿å­˜æœ€ç»ˆç»“æœ ---
    print("âœ… è®­ç»ƒç»“æŸã€‚ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
    model.save("./best_modelV4/final_model")
    try:
        model.save_replay_buffer("./best_modelV4/replay_buffer.pkl")
    except Exception as e:
        print(f"Bufferä¿å­˜å¤±è´¥: {e}")
```

è¯·å¸®æˆ‘å†™ä¸€ä¸ªå›æµ‹æ¥ç®€å•éªŒè¯