from datetime import datetime
from influx_manager import StockData, InfluxDBConfig, InfluxDBManager, InfluxDBCallbacks
import pandas as pd
from typing import Dict, List, Optional, Union
import glob
import os
from concurrent.futures import ThreadPoolExecutor
from multiprocessing import Manager
from concurrent.futures import ProcessPoolExecutor, as_completed
import time

# ------- é…ç½®ä¿¡æ¯ -------
HOST = "http://localhost:8181"
DATABASE = "stock_history_db"
TOKEN = "apiv3_DfumAJrYFgvwzRLausV9rI4_74-JlbekNQRlqf5gFT1wMnE4nc_ObRCNNtqtlynztO_pokRMII08bIhAbGoEyw"
# ------------------------



# å»ºè®®å°†è§£æå‡½æ•°æ”¾åœ¨å…¨å±€ä½œç”¨åŸŸï¼Œä»¥ä¾¿å¤šè¿›ç¨‹åºåˆ—åŒ–
def parse_single_csv(file_path: str):
    """
    è´Ÿè´£æœ€è€—æ—¶çš„è§£æå·¥ä½œï¼Œè¿è¡Œåœ¨ç‹¬ç«‹çš„è¿›ç¨‹ä¸­
    """
    try:
        df = pd.read_csv(file_path, encoding='utf-8')
        if df.empty: return []
        
        df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
        # å‘é‡åŒ–è½¬æ¢æ¯” iterrows å¿«å¾—å¤š
        stock_code = str(df['è‚¡ç¥¨ä»£ç '].iloc[0]).zfill(6)
        field_cols = ['å¼€ç›˜', 'æ”¶ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æˆäº¤é‡', 'æˆäº¤é¢', 'æŒ¯å¹…', 'æ¶¨è·Œå¹…', 'æ¶¨è·Œé¢', 'æ¢æ‰‹ç‡']
        
        # é¢„å…ˆè¿‡æ»¤æ‰å«ç©ºå€¼çš„åˆ—ï¼Œè½¬ä¸ºå­—å…¸åˆ—è¡¨ï¼Œå‡å°‘è·¨è¿›ç¨‹ä¼ è¾“å¼€é”€
        records = df.to_dict('records')
        batch_results = []
        for row in records:
            fields = {col: float(row[col]) for col in field_cols if pd.notna(row[col])}
            # ä¼ è¾“åŸºç¡€ç±»å‹å­—å…¸ï¼Œæ¯”ä¼ è¾“ StockData å¯¹è±¡æ›´è½»é‡
            batch_results.append({
                'time': row['æ—¥æœŸ'],
                'code': stock_code,
                'fields': fields
            })
        return batch_results
    except Exception as e:
        return []

if __name__ == '__main__':
    # --- å‚æ•°é…ç½® ---
    DIRECTORY = "C:/Users/sola/Documents/quant/history/past15year_stock_data_daily"
    BATCH_WRITE_SIZE = 500000  # å‡‘å¤Ÿ 50 ä¸‡è¡Œå†™ä¸€æ¬¡
    MAX_WORKERS = os.cpu_count()  # å……åˆ†åˆ©ç”¨æ‰€æœ‰æ ¸å¿ƒ
    
    # 1. åˆå§‹åŒ– InfluxDB
    config = InfluxDBConfig(HOST, DATABASE, TOKEN)
    manager = InfluxDBManager(config, InfluxDBCallbacks())
    
    csv_files = glob.glob(os.path.join(DIRECTORY, "*.csv"))
    # csv_files = [ DIRECTORY + "/index_sh000001.csv"]
    print(f"ğŸ”¥ å¯åŠ¨å¤šè¿›ç¨‹è§£æå¼•æ“ (Workers: {MAX_WORKERS})...")
    
    pending_buffer = []
    total_count = 0
    start_time = time.time()

    # 2. ä½¿ç”¨è¿›ç¨‹æ± å¹¶è¡Œè§£æ
    with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # as_completed ä¿è¯å“ªä¸ªæ–‡ä»¶å…ˆè§£æå®Œå°±å…ˆå¤„ç†å“ªä¸ªï¼Œä¸å†æŒ‰é¡ºåºæ­»ç­‰
        future_to_file = {executor.submit(parse_single_csv, f): f for f in csv_files}
        
        for future in as_completed(future_to_file):
            data = future.result()
            if data:
                pending_buffer.extend(data)
                
                # 3. è¾¾åˆ°æ‰¹æ¬¡å¤§å°ï¼Œå¼‚æ­¥å†™å…¥ IO
                if len(pending_buffer) >= BATCH_WRITE_SIZE:
                    # è½¬æ¢ä¸º StockData å¯¹è±¡
                    write_list = [StockData(timestamp=d['time'], stock_code=d['code'], fields=d['fields']) 
                                 for d in pending_buffer]
                    print(f"ğŸ“¦ ç¼“å†²åŒºå·²æ»¡ ({len(write_list)}è¡Œ)ï¼Œæ­£åœ¨æäº¤ InfluxDB...")
                    manager.write_stock_batch(write_list)
                    total_count += len(write_list)
                    pending_buffer = []

    # 4. æ¸…ç©ºå‰©ä½™æ•°æ®
    if pending_buffer:
        write_list = [StockData(timestamp=d['time'], stock_code=d['code'], fields=d['fields']) 
                     for d in pending_buffer]
        manager.write_stock_batch(write_list)
        total_count += len(write_list)

    end_time = time.time()
    manager.close()
    print(f"\nâœ¨ é‡æ„å®Œæˆï¼")
    print(f"â±ï¸ æ€»è€—æ—¶: {end_time - start_time:.2f} ç§’")
    print(f"ğŸ“ˆ æ€»è®°å½•æ•°: {total_count}")